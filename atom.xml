<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>My Trantor</title>
  
  <subtitle>My Life of Code, Math and Literature</subtitle>
  <link href="https://huangweiran.club/atom.xml" rel="self"/>
  
  <link href="https://huangweiran.club/"/>
  <updated>2021-08-28T10:51:52.000Z</updated>
  <id>https://huangweiran.club/</id>
  
  <author>
    <name>Weiran Huang</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>玩具项目计划</title>
    <link href="https://huangweiran.club/2021/08/05/%E7%8E%A9%E5%85%B7%E9%A1%B9%E7%9B%AE%E8%AE%A1%E5%88%92/"/>
    <id>https://huangweiran.club/2021/08/05/%E7%8E%A9%E5%85%B7%E9%A1%B9%E7%9B%AE%E8%AE%A1%E5%88%92/</id>
    <published>2021-08-05T17:00:11.000Z</published>
    <updated>2021-08-28T10:51:52.000Z</updated>
    
    <content type="html"><![CDATA[<p>最近觉得有必要在日常工作之余写一些娱乐为主的toy project，一来是给无聊的生活找找乐子，二来也保持一下自己的编程手感。</p><p>选择要写什么项目主要有两点原则：第一，既然是“toy”，那么就必须得好玩；第二，最好能锻炼或者学习一些之前不熟或者根本不会的新技能。</p><p>目前我已经写完了前两个项目。这篇博客的目的就是记录下这些项目的思路、实现过程、技术难点等。希望可以写非常长，但目前为止我立的flag好像从未实现过。</p><h2 id="ANSI-art"><a href="#ANSI-art" class="headerlink" title="ANSI-art"></a>ANSI-art</h2><p>这个项目的代码已经开源在Github上，使用的编程语言是我最近很喜欢的Go，地址在<a href="https://github.com/EtoDemerzel0427/ANSI-art" target="_blank" rel="noopener">这里</a>。</p><p>它的主要功能是让一张这样的图片:</p><p><img src="https://github.com/EtoDemerzel0427/ANSI-art/blob/master/pic/messi.png?raw=true" alt="messi.png"></p><p>变成这样：</p><p><img src="https://github.com/EtoDemerzel0427/ANSI-art/raw/master/pic/messi_output.png" alt="messi_output" style="zoom: 33%;"></p><p>或者这样：</p><p><img src="https://github.com/EtoDemerzel0427/ANSI-art/raw/master/pic/messi_ansi.png" alt="messi_ansi" style="zoom:33%;"></p><p>或者这样：</p><p><img src="https://github.com/EtoDemerzel0427/ANSI-art/raw/master/pic/messi_block.png" alt="messi_block" style="zoom:33%;"></p><p>要知道以上这些画面都出现在你黑漆漆的命令行！</p><p>更棒的是，如果你能对图片做这样的事，很自然地你会想到，我们为什么不能对视频这么做呢？显然我犯懒了，目前尚未支持视频（也许永远不会），但我们已经有了Gif版本：</p><iframe width="640" height="480" src="https://user-images.githubusercontent.com/32998901/126824445-57629942-fe1c-4ccd-9b72-3f2a1e564cf3.mp4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen><br></iframe><p>在命令行里播放动画是多么酷的一件事啊（希望你也这么觉得）。</p><p>为了让它更酷一点，我甚至通过<a href="https://github.com/faiface/beep" target="_blank" rel="noopener">beep</a>添加了背景音乐功能，于是你的命令行不仅可以播放动画，还能播放音乐。</p><p>这就是这个项目的大概内容。我还有几个想加的功能，但不对加上它们的时间做任何保证，比如把生成的图片和动画保存成文件形式；支持mp4等格式；以及把这个项目放在Web上。（这些功能并没有什么难度，只是继续为这个项目增加功能带给我技术上的收益以及情绪价值已经不足了。）</p><p>接下来谈谈这个项目的实现。这个项目最有趣的部分应该是ASCII版本，因为用字符去绘制一张图片看起来真的挺酷的——它酷的地方在于，你第一眼看到是真的会想：这是怎么做到的？相比之下，ANSI版本看上去就有些显然了，提取每个像素的RGB值，给对应位置的字符通过转义字符设定颜色。当然，很多人可能不知道如何在命令行给字符设定颜色，但如果你看过<a href="https://en.wikipedia.org/wiki/ANSI_escape_code" target="_blank" rel="noopener">这个Wikipedia主页</a>，你就会知道做到这件事是多么简单，因此我不会详细介绍ANSI版本的实现，对它们来说也许Code is the best documentation。</p><p>不过尽管也许你第一眼无法看出ASCII版本是如何实现的，它的原理实际上也并不复杂。在这里我们依然用字符来表示每一个像素，ASCII字符中printable character是从32到126的95个字符，因为表达能力有限，我们用它们来代表不同的灰度值（共128个），所以在ASCII模式下，每张输入的图片首先要通过grayscale转化为黑白图片。接下来要做的就是定义映射规则，如何将0-127的灰度值映射到26-126这些ASCII字符上。</p><p>这个问题的答案也很简单，我们来举一个例子。在一个等宽字体中，我们对比<code>M</code>和<code>.</code>,显然<code>M</code>比<code>.</code>更接近黑色，这是因为在同样一个单元格内，<code>M</code>的黑色像素数目要比<code>.</code>多。于是很自然地，我们会想到，如果可以按黑色像素数量给ASCII字符排序，那么这个映射规则就很容易得到了。</p><p>这个排序可以做得简单粗暴。我们将每个printable的ASCII字符画在白底图片上，去数所有RGB值非（255，255，255）的像素数量就可以了。这部分代码在rank文件夹下。值得一提的是，虽然大多数字符在不同字体下看起来差不多，但这个排序的确是font-specific的。我terminal的字体是monaco，因此在这里我的排序也是基于monaco字体的，如果使用其他字体，最好下载其对应的tff文件重新跑一遍rank。绘制字符并计算非白像素的代码如下：</p><pre class=" language-go"><code class="language-go">    awidth<span class="token punctuation">,</span> <span class="token boolean">_</span> <span class="token operator">:=</span> face<span class="token punctuation">.</span><span class="token function">GlyphAdvance</span><span class="token punctuation">(</span>text<span class="token punctuation">)</span>    iwidthf <span class="token operator">:=</span> <span class="token function">int</span><span class="token punctuation">(</span><span class="token function">float64</span><span class="token punctuation">(</span>awidth<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">64</span><span class="token punctuation">)</span>    pt <span class="token operator">:=</span> freetype<span class="token punctuation">.</span><span class="token function">Pt</span><span class="token punctuation">(</span><span class="token number">125</span><span class="token operator">-</span>iwidthf<span class="token operator">/</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span>    <span class="token boolean">_</span><span class="token punctuation">,</span> <span class="token boolean">_</span> <span class="token operator">=</span> c<span class="token punctuation">.</span><span class="token function">DrawString</span><span class="token punctuation">(</span><span class="token function">string</span><span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">,</span> pt<span class="token punctuation">)</span>    size <span class="token operator">:=</span> rgba<span class="token punctuation">.</span><span class="token function">Bounds</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">Size</span><span class="token punctuation">(</span><span class="token punctuation">)</span>    blackCnt <span class="token operator">:=</span> <span class="token number">0</span>    <span class="token keyword">for</span> i <span class="token operator">:=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> size<span class="token punctuation">.</span>X<span class="token punctuation">;</span> i<span class="token operator">++</span> <span class="token punctuation">{</span>        <span class="token keyword">for</span> j <span class="token operator">:=</span> <span class="token number">0</span><span class="token punctuation">;</span> j <span class="token operator">&lt;</span> size<span class="token punctuation">.</span>Y<span class="token punctuation">;</span> j<span class="token operator">++</span> <span class="token punctuation">{</span>            pixel <span class="token operator">:=</span> rgba<span class="token punctuation">.</span><span class="token function">At</span><span class="token punctuation">(</span>i<span class="token punctuation">,</span> j<span class="token punctuation">)</span>            r<span class="token punctuation">,</span> g<span class="token punctuation">,</span> b<span class="token punctuation">,</span> a <span class="token operator">:=</span> pixel<span class="token punctuation">.</span><span class="token function">RGBA</span><span class="token punctuation">(</span><span class="token punctuation">)</span>            r <span class="token operator">>>=</span> <span class="token number">8</span>            g <span class="token operator">>>=</span> <span class="token number">8</span>            b <span class="token operator">>>=</span> <span class="token number">8</span>            a <span class="token operator">>>=</span> <span class="token number">8</span>            <span class="token keyword">if</span> r <span class="token operator">!=</span> <span class="token number">255</span> <span class="token operator">||</span> g <span class="token operator">!=</span> <span class="token number">255</span> <span class="token operator">||</span> b <span class="token operator">!=</span> <span class="token number">255</span> <span class="token punctuation">{</span>                blackCnt<span class="token operator">++</span>            <span class="token punctuation">}</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span></code></pre><p>这样排序得到的值实际上是黑色像素的个数，为了映射，我们通过<code>(value - min_value)/(max_value - min_value) * 128</code>的方式把数值的range控制在[0,128]。这样，当我们拿到一个像素的灰度值后，只需要在这个有序序列里找到最接近的值就行了，这个显然可以通过二分查找做到。这部分代码如下：</p><pre class=" language-go"><code class="language-go"><span class="token keyword">func</span> <span class="token function">findClosestK</span><span class="token punctuation">(</span>value <span class="token builtin">int</span><span class="token punctuation">,</span> arr <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token builtin">float64</span><span class="token punctuation">)</span> <span class="token punctuation">(</span>id <span class="token builtin">int</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>    x <span class="token operator">:=</span> <span class="token function">float64</span><span class="token punctuation">(</span>value<span class="token punctuation">)</span>    low<span class="token punctuation">,</span> high<span class="token punctuation">,</span> mid <span class="token operator">:=</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token function">len</span><span class="token punctuation">(</span>arr<span class="token punctuation">)</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span>    <span class="token keyword">for</span> low <span class="token operator">&lt;</span> high <span class="token punctuation">{</span>        mid <span class="token operator">=</span> low <span class="token operator">+</span> <span class="token punctuation">(</span>high<span class="token operator">-</span>low<span class="token punctuation">)</span><span class="token operator">/</span><span class="token number">2</span>        <span class="token keyword">if</span> arr<span class="token punctuation">[</span>mid<span class="token punctuation">]</span> <span class="token operator">==</span> x <span class="token punctuation">{</span>            <span class="token comment" spellcheck="true">// this is very unlikely for floats, but we still keep this</span>            <span class="token keyword">return</span> mid        <span class="token punctuation">}</span>        <span class="token keyword">if</span> x <span class="token operator">&lt;</span> arr<span class="token punctuation">[</span>mid<span class="token punctuation">]</span> <span class="token punctuation">{</span>            <span class="token keyword">if</span> mid <span class="token operator">></span> <span class="token number">0</span> <span class="token operator">&amp;&amp;</span> x <span class="token operator">></span> arr<span class="token punctuation">[</span>mid<span class="token number">-1</span><span class="token punctuation">]</span> <span class="token punctuation">{</span>                d1 <span class="token operator">:=</span> math<span class="token punctuation">.</span><span class="token function">Abs</span><span class="token punctuation">(</span>arr<span class="token punctuation">[</span>mid<span class="token punctuation">]</span> <span class="token operator">-</span> x<span class="token punctuation">)</span>                d2 <span class="token operator">:=</span> math<span class="token punctuation">.</span><span class="token function">Abs</span><span class="token punctuation">(</span>arr<span class="token punctuation">[</span>mid<span class="token number">-1</span><span class="token punctuation">]</span> <span class="token operator">-</span> x<span class="token punctuation">)</span>                <span class="token keyword">if</span> d1 <span class="token operator">&lt;</span> d2 <span class="token punctuation">{</span>                    <span class="token keyword">return</span> mid                <span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token punctuation">{</span>                    <span class="token keyword">return</span> mid <span class="token operator">-</span> <span class="token number">1</span>                <span class="token punctuation">}</span>            <span class="token punctuation">}</span>            high <span class="token operator">=</span> mid        <span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token punctuation">{</span>            <span class="token keyword">if</span> mid <span class="token operator">&lt;</span> <span class="token function">len</span><span class="token punctuation">(</span>arr<span class="token punctuation">)</span><span class="token operator">-</span><span class="token number">1</span> <span class="token operator">&amp;&amp;</span> x <span class="token operator">&lt;</span> arr<span class="token punctuation">[</span>mid<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token punctuation">{</span>                d1 <span class="token operator">:=</span> math<span class="token punctuation">.</span><span class="token function">Abs</span><span class="token punctuation">(</span>arr<span class="token punctuation">[</span>mid<span class="token punctuation">]</span> <span class="token operator">-</span> x<span class="token punctuation">)</span>                d2 <span class="token operator">:=</span> math<span class="token punctuation">.</span><span class="token function">Abs</span><span class="token punctuation">(</span>arr<span class="token punctuation">[</span>mid<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">-</span> x<span class="token punctuation">)</span>                <span class="token keyword">if</span> d1 <span class="token operator">&lt;</span> d2 <span class="token punctuation">{</span>                    <span class="token keyword">return</span> mid                <span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token punctuation">{</span>                    <span class="token keyword">return</span> mid <span class="token operator">+</span> <span class="token number">1</span>                <span class="token punctuation">}</span>            <span class="token punctuation">}</span>            low <span class="token operator">=</span> mid <span class="token operator">+</span> <span class="token number">1</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span>    <span class="token keyword">return</span> low<span class="token punctuation">}</span></code></pre><p>就这样我们完成了由灰度值到ASCII字符的映射。</p><p>当然还有其他的一些事情可以做，比如对于一些颜色上不太适合的图片可能需要调节contrast或者进行sharpen后才能获得更好的效果，我们把具体的设置放在命令行参数里由用户自行决定。</p><p>在实现过程中，其实也遇到了一些其他问题。其中一个问题在于文件路径。在排序中，我是将排序结果输出到文件；在转化为ASCII/ANSI版本的时候，通过读文件的方式来获得排序。那么这个文件路径就是一个有一点麻烦的地方：对我的<code>main.go</code>文件来说，自然是很容易设置这个相对路径；但如果有人要import我的包，这个路径就不对了。目前在go module开启的情况下，引用的包的下载路径是<code>$GOPATH/pkg/mod</code>。但这样我并不能区分是在内部的<code>main.go</code>调用还是以module的形式调用，因此我决定通过<code>runtime</code>来确定目前的具体路径，一劳永逸：</p><pre class=" language-go"><code class="language-go"><span class="token keyword">var</span> <span class="token punctuation">(</span>    <span class="token boolean">_</span><span class="token punctuation">,</span> b<span class="token punctuation">,</span> <span class="token boolean">_</span><span class="token punctuation">,</span> <span class="token boolean">_</span> <span class="token operator">=</span> runtime<span class="token punctuation">.</span><span class="token function">Caller</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">// Root folder of this project</span>    Root <span class="token operator">=</span> filepath<span class="token punctuation">.</span><span class="token function">Join</span><span class="token punctuation">(</span>filepath<span class="token punctuation">.</span><span class="token function">Dir</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"../"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">// the intensity/rank files are predefined</span>    intensity<span class="token punctuation">,</span> <span class="token boolean">_</span> <span class="token operator">:=</span> <span class="token function">readFloatLines</span><span class="token punctuation">(</span>Root <span class="token operator">+</span> <span class="token string">"/rank/intensity.txt"</span><span class="token punctuation">)</span>    rank<span class="token punctuation">,</span> <span class="token boolean">_</span> <span class="token operator">:=</span> <span class="token function">readIntLines</span><span class="token punctuation">(</span>Root <span class="token operator">+</span> <span class="token string">"/rank/rank.txt"</span><span class="token punctuation">)</span></code></pre><p>这个项目的一大遗憾是，写它的时候我对<a href="https://github.com/gdamore/tcell" target="_blank" rel="noopener">tcell</a>一无所知，是直接将结果输出到终端的。当ANSI转义字符（需要渲染）很多的时候，由于有时会很明显地在动画中观察到闪烁。我通过调慢帧率一定程度上缓解了这个问题，但并没有解决。而tcell这样的终端模拟器因为自己维护了intermediate buffer，在这方面可以自由优化（比如只重新渲染相邻帧不同的部分，当然这只是我的猜测，我并没有真的读过它的源码）；而直接输出到真正的终端，显然是无法对它的buffer做任何操作的。为了弥补这个遗憾（如前所述，我已经懒得继续修改这个项目），我在下一个ASCII-Live中使用了tcell。</p><h2 id="ASCII-Live"><a href="#ASCII-Live" class="headerlink" title="ASCII-Live"></a>ASCII-Live</h2><p>ASCII-Live是我花了半天时间写完的一个demo project，效果是将Webcam捕捉的视频实时以ASCII-art的形式呈现在终端：</p><iframe width="640" height="480" src="https://user-images.githubusercontent.com/32998901/130675904-b090e856-af42-4c07-b732-84ca9127cb14.mp4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen><br></iframe><p>这个项目显然是基于ANSI-art的（这也是为什么刚才我要花时间去解决路径的问题），算是它的一个小应用，灵感来源于<a href="https://github.com/esimov/ascii-fluid" target="_blank" rel="noopener">ascii-fluid</a>。项目的思路非常简单，前端每0.1s捕捉一帧画面传到后端，再在后端调用ANSI-art的函数，实时输出到终端。</p><p>这里简单介绍一下这个项目的技术栈：</p><ul><li>前端使用React完成，主要依赖<a href="https://github.com/mozmorris/react-webcam" target="_blank" rel="noopener">React-webcam</a>。开始的时候考虑过不用浏览器，直接通过Go打开webcam，但Google了一下没有找到什么很好的方案，大部分都要依赖Go版本的OpenCV。因为这实在是一个沉重的依赖，并且很有点杀鸡用牛刀的感觉，我放弃了这个想法。必须承认，React我是临时学的，确实很好上手。</li><li>前后端通信我使用了WebSocket。WebSocket的特点是可以维护一个长时间的双向全双工固定连接，相对于HTTP的长轮询，WebSocket能减少很多开销。在Client端，我们只需要一行<code>const socket = new WebSocket(&quot;ws://localhost:8080/ws&quot;);</code>即可；而在Server端，使用<a href="https://github.com/gorilla/websocket" target="_blank" rel="noopener">Gorilla/WebSocket</a>，也可以很容易地建立起WebSocket连接。</li><li>这次画在终端上使用了tcell。</li></ul><p>其他的部分就乏善可陈了，毕竟这个项目本身也只是ANSI-art的一个demo。但在写这个项目的过程中，有一件比较有意思的事情值得聊聊。</p><p>原本前端部分我并不打算用React。看到ascii-fluid使用的Go-WebAssembly之后，我原本打算对他的代码照（直）猫（接）画（复）虎（用），顺便了解一下wasm。而原作者自己看上去非常喜欢Go的WebAssembly方案（尽管性能堪忧），在他维护的数个repo中都使用了同一段go-WebAssembly代码来调用Webcam，而他也一直没有意识到这之间藏着的巨大隐患。</p><p>Go的wasm学起来就很不舒服，因为写起来几乎就像是在写js的go binding，没有我想象中更高抽象度的封装，因此写起来的体验也许甚至不如纯js。</p><p>而更可怕的是，我很快就发现了原作者的一个致命bug：打开webcam后大概一分钟，浏览器的内存狂飙几个G，然后网页直接crash。</p><p>采用了经典的注释Debug法后，我把问题定位到了syscall/js提供的CopyBytesToGo上。在实现里我们以几十FPS的频率将webcam的帧数据从前端传到后端，在这个过程中需要用CopyBytesToGo将js的uint8array转换为Go的slices。</p><p>和原作者讨论后，我们觉得问题出在GC上。像Go这样的自动GC语言让编程者不用亲自处理内存的申请和释放，但另一方面也让编程者失去了对内存的直接控制。就这个特定场景而言，似乎是因为我们无法控制内存释放的时间。</p><p>假设我们以每秒60帧从webcam抽取640x480的图片，那么一秒钟我们会有70多MB数据，如果这些内存不被及时释放，一分钟时间内将需要4GB以上的开销。这也和我观察的一致：运行期间我通过activity monitor看到在程序崩掉的一分钟内，浏览器内存占用从2GB暴增到6GB多。——显然这个“如果”已经成真，但由于Go是自动GC，我们甚至无法手动释放这部分内存。</p><p>刚才提到那位原作者维护了数个重用了这部分代码的repo，其中有一个甚至有3k+ star，但在我发现这个问题之前他从未意识到存在这样一个bug。尝试优化了几天之后，他意识到自己无法解决它，因此这个问题成为了Golang官方repo下的一个issue，而我转头学React去了。</p><p>自动GC语言的弊端并不罕见。去年二月，Discord宣布全面从Go语言转为Rust（如图一）。因为他们发现由于Go每两分钟的GC，他们服务的延迟和CPU的使用率每两分钟都要经历一次spike；而切换了无GC性能又好的Rust之后，问题解决了（如图二，蓝色的是Rust，紫色的是Go）。无独有偶，LinkedIn也在2016年发博客表示因为JVM的GC logging被后台IO阻塞，他们许多在线服务的线程全都被迫停止（如图三）。</p><p><img src="https://i.loli.net/2021/08/28/Mv2pkbzwaN87SJj.png" alt="351630091419_.pic_hd.jpg" style="zoom: 33%;"></p><p><img src="https://i.loli.net/2021/08/28/15EjKnu3XOgZpkI.jpg" alt="331630091370_.pic.jpg" style="zoom: 67%;"></p><p><img src="https://i.loli.net/2021/08/28/p1mQCb5aJsML48z.png" alt="361630091474_.pic_hd.jpg" style="zoom:33%;"></p><p>C语言的内存泄露问题由来已久。C++试图通过RAII（智能指针，STL）解决，但被迫向前兼容，积重难返；Go和Java试图通过自动垃圾回收来规避问题，但在对性能要求高的场景下往往因为额外的runtime以及难以对内存精细控制而捉襟见肘；也许最佳的方案来自Rust：把RAII的思想推广到语言层面（ownership和borrow），在编译期最大程度上杜绝原本在运行时才会暴露的内存错误。出于这个原因，我决定下一个项目使用Rust来写，尽管更新这篇博客的此时此刻，我仍然处在Rust学习的初级阶段。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;最近觉得有必要在日常工作之余写一些娱乐为主的toy project，一来是给无聊的生活找找乐子，二来也保持一下自己的编程手感。&lt;/p&gt;
&lt;p&gt;选择要写什么项目主要有两点原则：第一，既然是“toy”，那么就必须得好玩；第二，最好能锻炼或者学习一些之前不熟或者根本不会的新技能。</summary>
      
    
    
    
    <category term="tech" scheme="https://huangweiran.club/categories/tech/"/>
    
    
    <category term="project" scheme="https://huangweiran.club/tags/project/"/>
    
    <category term="coding" scheme="https://huangweiran.club/tags/coding/"/>
    
  </entry>
  
  <entry>
    <title>写在前面</title>
    <link href="https://huangweiran.club/2021/06/29/%E5%86%99%E5%9C%A8%E5%89%8D%E9%9D%A2/"/>
    <id>https://huangweiran.club/2021/06/29/%E5%86%99%E5%9C%A8%E5%89%8D%E9%9D%A2/</id>
    <published>2021-06-29T07:58:19.000Z</published>
    <updated>2021-06-29T08:39:29.000Z</updated>
    
    <content type="html"><![CDATA[<p>在我意识到我什么也不会之后，我很久没有在博客上写过什么东西。<br><a id="more"></a></p><p>这个博客大约是我在大二上学期的时候建立的，现在回头看，当时的确写了不少现在看起来稍显丢人的东西。我并不对大二的我过分苛责，因为未来的我看现在的我应该也是一样的感受，但我还是不能免俗把其中我觉得丢人的那些博文隐藏了。</p><p>之前这些博文会记录一些零散、不成体系的知识点性质的内容，但2018年以后我主要在<a href="https://huangweiran.club/wiki/">TiddlyWiki</a>上记录这些东西。2020年以来我开始使用Notion作为个人知识库。Notion在我看来是比TiddlyWiki强大很多的工具，并且好像已经有人找到办法<a href="https://github.com/ijjk/notion-blog" target="_blank" rel="noopener">把它部署到Github Pages上</a>，但我尚无动力去仔细了解并使用。</p><p>至于真正能以文章形式呈现的博客，我是很久没有写过了。回看2018年写过的两篇读书笔记，觉得彼时的自己精神浪漫得让人嫉妒。至于技术类的文章，以前是完全只能拾人牙慧，没有自己的东西，写出来食之无味弃之可惜；现在知道得比以前多了（当然也只是相对而言），但不如从前那样动力十足，求学若渴了。</p><p>希望我之后还会有更新博客的一天。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;在我意识到我什么也不会之后，我很久没有在博客上写过什么东西。&lt;br&gt;</summary>
    
    
    
    <category term="all" scheme="https://huangweiran.club/categories/all/"/>
    
    
    <category term="links" scheme="https://huangweiran.club/tags/links/"/>
    
  </entry>
  
  <entry>
    <title>Bash Tutorial</title>
    <link href="https://huangweiran.club/2019/11/14/Bash-Tutorial/"/>
    <id>https://huangweiran.club/2019/11/14/Bash-Tutorial/</id>
    <published>2019-11-14T23:35:13.000Z</published>
    <updated>2020-01-30T13:39:46.000Z</updated>
    
    <content type="html"><![CDATA[<p>对程序员而言，Shell是一种必备技能。目前，Bash是最流行的Shell程序之一，因此学会Bash的使用是很有必要的。</p><h2 id="Step-01"><a href="#Step-01" class="headerlink" title="Step 01"></a>Step 01</h2><h3 id="什么是Bash？"><a href="#什么是Bash？" class="headerlink" title="什么是Bash？"></a>什么是Bash？</h3><p>Bash是“Bourne Again SHell”的简称，从名字来看，它是一个Shell程序，并且和Bourne（也是一种Shell）有着某种继承关系。</p><h3 id="什么是Shell？"><a href="#什么是Shell？" class="headerlink" title="什么是Shell？"></a>什么是Shell？</h3><p>Shell是一种可执行的二进制文件，它接收你输入在终端的命令，并最终将这些命令翻译为系统调用，借助你的操作系统提供的API完成一系列工作。在中文里，Shell的意思是“外壳”：你可以把它想象成套在操作系统上的一层外壳，你所有的操作都在外壳上进行，但实际发挥作用的是外壳里的东西，就像是你通过外壳上的按钮来操作一台电视机一样。</p><h3 id="除了Bash之外？"><a href="#除了Bash之外？" class="headerlink" title="除了Bash之外？"></a>除了Bash之外？</h3><p>除了Bash之外当然还有很多其他的Shell程序，比如我自己使用的就是zsh。此外还有sh（就是刚才说的Bourne），ash，dash，ksh等等</p><p>你可以通过以下这张图片了解他们之间的关系：</p><p><img src="https://s2.ax1x.com/2019/11/14/MNtqNF.png" alt="MNtqNF.png"></p><p>一件奇特的事情是你可以在某一个Shell里运行另一个Shell：不过这种特性是很自然的，因为“将运行其他程序统一为一体”正是Shell的一大特性，这里不过是把另一个Shell当作另一个程序罢了。</p><p>这个特色让我们得以轻松比较不同的Shell。比如在zsh，tcsh等中，<code>$dirstack</code>变量会存储最近访问过的路径列表，但这个变量在Bash中不存在：</p><p><img src="https://s2.ax1x.com/2019/11/14/MNG5X6.md.png" alt="MNG5X6.md.png"></p><p>（我在tcsh和zsh中<code>$dirstack</code>的值不同是因为我在两个shell里访问过的路径不同）</p><p>尽管如此，它们在很多时候是相通的。</p><h2 id="Step-02"><a href="#Step-02" class="headerlink" title="Step 02"></a>Step 02</h2><h3 id="Globbing：通配符"><a href="#Globbing：通配符" class="headerlink" title="Globbing：通配符"></a>Globbing：通配符</h3><p>通配符的英文是wildcards，又叫做globbing patterns，它用来处理一组路径，可以被看做早期的正则表达式。之所以叫glob是因为Unix早期有一个<code>/etc/glob</code>文件来保存通配符模板。它现在被内置在Bash中，Python中也有一个glob模块。</p><p>一个简单的例子如下所示，和正则式有相通之处，这里不详细讲解：</p><p><img src="https://s2.ax1x.com/2019/11/15/Maixmt.md.png" alt="Maixmt.md.png"></p><p><strong>但是你会发现，如果你输入的是<code>ls &#39;*&#39;</code>和<code>echo &#39;*&#39;</code>，这里的<code>*</code>并没有发生任何匹配，而是直接被当作这个字符来处理了。</strong><br>当然正则式和glob的区别还是存在的并且不少，而且Bash本身也对glob做了一些拓展。这里不再细究。</p><h3 id="Variables：变量"><a href="#Variables：变量" class="headerlink" title="Variables：变量"></a>Variables：变量</h3><pre class=" language-bash"><code class="language-bash">MYSTRING<span class="token operator">=</span>astring<span class="token keyword">echo</span> <span class="token variable">$MYSTRING</span></code></pre><p>非常容易理解，值得注意的是取值的时候需要加上<code>$</code>，并且是不要乱加空格。如果变量值内部有空格，就需要引号，这里单引号双引号都可以：</p><pre class=" language-bash"><code class="language-bash">MYSTRING<span class="token operator">=</span><span class="token string">"a sentence"</span></code></pre><p>但单双引号的一大区别是，当你在引号里加入变量名的时候：</p><pre class=" language-bash"><code class="language-bash">MYSENTENCE<span class="token operator">=</span><span class="token string">"This is <span class="token variable">$MYSTRING</span>"</span><span class="token keyword">echo</span> <span class="token variable">$MYSENTENCE</span>  <span class="token comment" spellcheck="true"># OUTPUT: This is astring</span>MYSENTENCE<span class="token operator">=</span><span class="token string">'This is <span class="token variable">$MYSTRING</span>'</span><span class="token keyword">echo</span> <span class="token variable">$MYSENTENCE</span>  <span class="token comment" spellcheck="true"># OUTPUT: This is $MYSTRING</span></code></pre><p>前者会取变量的值，而后者不会。</p><p>事实上Bash对待变量和单双引号这个事情的效果还挺有趣的，而且和我通常使用的zsh有所不同：</p><p><img src="https://s2.ax1x.com/2019/11/15/MaEEXd.md.png" alt="MaEEXd.md.png"></p><p>可以看到，无论<code>MYGLOB</code>被赋值<code>*</code>还是<code>&quot;*&quot;</code>，存在<code>MYGLOB</code>里的都是一个单独的字符；并且在Bash中变量的值还可以进一步被当作通配符被解析，而zsh则不会。</p><p>Shell本身自带一些变量，比如<code>PPID</code>，它储存着当前Bash的父进程ID，这种变量是只读的。我们自己也可以设置只读变量：</p><pre class=" language-bash"><code class="language-bash"><span class="token function">readonly</span> MYVAR<span class="token operator">=</span>varMYVAR<span class="token operator">=</span>antohervar  <span class="token comment" spellcheck="true"># error</span></code></pre><p>还有一个问题。当你设置了一个变量，又打开了另一个进程，在新的进程里你的变量就不存在了。但如果你使用<code>export</code> 命令，这个变量就会出现在你当前shell中运行的所有进程的环境中了：</p><pre class=" language-bash"><code class="language-bash"><span class="token function">export</span> MYSTRING<span class="token operator">=</span>astring</code></pre><p>变量还可以是array或associate array的形式。这里不详细介绍。</p><h3 id="Functions-函数"><a href="#Functions-函数" class="headerlink" title="Functions: 函数"></a>Functions: 函数</h3><p>像一个编程语言一样，Shell还可以编写函数：</p><pre class=" language-bash"><code class="language-bash"><span class="token keyword">function</span> myfunc <span class="token punctuation">{</span>    <span class="token keyword">echo</span> <span class="token variable">$1</span>    <span class="token keyword">echo</span> <span class="token variable">$2</span><span class="token punctuation">}</span></code></pre><p>这个声明和普通的编程语言没什么区别，主要的不同就是它并不指出参数，并且它也并不在意你到底给它几个参数，因为多余的它会忽略，缺少的它会认为是空：</p><pre class=" language-shell"><code class="language-shell">myfunc hello world haha  # output: hello worldmyfunc hello world       # output: hello worldmyfunc hello             # output: hello(and an empty line)</code></pre><p>Shell函数中的变量也有其作用域。外部变量在函数中都可见，而想要函数内部的变量不被外部可见，可以在声明前加上<code>local</code>。</p><h2 id="Step-03"><a href="#Step-03" class="headerlink" title="Step 03"></a>Step 03</h2><h3 id="Commands-命令"><a href="#Commands-命令" class="headerlink" title="Commands: 命令"></a>Commands: 命令</h3><p>Bash中的命令主要是四种：</p><ul><li>内置（builtins）</li><li>函数（functions）</li><li>程序（programs）</li><li>别名（Aliases）</li></ul><p>内置命令是指直接在Bash里定义好的命令，最常见的是<code>cd</code>，<code>builtin</code>自己也是一个builtin。当你输入<code>builtin cd /dir</code> 的时候它会和普通的<code>cd</code>一样运行；而如果是一个非builtin的命令，则会被提示没有这个命令。</p><p>一个它可能会发挥作用的场景是：假如你定义了一个函数也叫<code>cd</code>，那么它会覆盖原来builtin的<code>cd</code>，这时候输入<code>builtin cd /dir</code> 还是会唤起builtin的版本。</p><p>说到函数，要查看当前环境下的所有函数可以使用：</p><pre class=" language-bash"><code class="language-bash"><span class="token keyword">declare</span> -f  <span class="token comment" spellcheck="true"># show all the functions in the environment</span><span class="token keyword">declare</span> -F  <span class="token comment" spellcheck="true"># only show their names</span></code></pre><p>对于已经定义的函数，可以通过<code>unset -f</code>来解除。</p><p>程序，在这里指可执行的文件。包括<code>grep</code>，<code>sed</code>，<code>vi</code> 等都是常见的程序。要判断一个命令到底是builtin还是program，可以通过刚才说的，在命令前加上builtin，也可以使用<code>which &lt;command&gt;</code>查看命令是否保存在文件系统中来确定。</p><p><code>which</code> 自身是一个builtin。当你输入<code>which which</code> 时，可以看到这个结果。</p><p>别名很容易理解，就是一个字符串，你输入shell之后它会自动把你翻译给这个别名所指代的另一个字符串，这个字符串可能是一个命令。</p><p>用法是：</p><pre class=" language-bash"><code class="language-bash"><span class="token function">alias</span> a<span class="token operator">=</span>b  <span class="token comment" spellcheck="true"># make alias</span><span class="token function">unalias</span> a  <span class="token comment" spellcheck="true"># delete this alias</span><span class="token function">alias</span>      <span class="token comment" spellcheck="true"># show alias available</span></code></pre><p>在我的zsh中，<code>ls</code>是<code>ls -G</code>的别名。<code>ls -G</code>中的<code>ls</code>是一个程序，这里被这个别名给覆盖了。</p><h3 id="Pipes-and-Redirects：管道和重定向"><a href="#Pipes-and-Redirects：管道和重定向" class="headerlink" title="Pipes and Redirects：管道和重定向"></a>Pipes and Redirects：管道和重定向</h3><p>重定向符号是<code>&gt;</code>：</p><pre><code>echo &quot;content of file1&quot; &gt; file1</code></pre><p>它把前一个指令的输出发送到后面的文件中；如果文件不存在它会创建一个。默认的输出是标准输出流，如果要输出标准错误流，则需要在<code>&gt;</code>前加上文件描述符（0是标准输入流，1是标准输出流，2是标准错误流），默认是1，标准输出流：</p><pre class=" language-bash"><code class="language-bash"><span class="token function">command</span> 2<span class="token operator">></span> /dev/null</code></pre><p>顺带一提，<code>/dev/null</code>是一个特殊的文件，传给它的所有东西都会被直接丢弃。</p><p>我们知道标准输出流和标准错误流是分别独立存在的。有时当我们写程序，并且程序有输出、又出现报错信息的时候，我们会发现它们是交替出现的，这正是这一特性的体现。但我们可以将标准错误流重定向到标准输出流指向的地方：</p><pre class=" language-bash"><code class="language-bash">command_does_not_exist 2<span class="token operator">></span><span class="token operator">&amp;</span>1</code></pre><p>通常来说它们两个都是指向终端，所以没有什么区别，但是它们中有人被重定向到文件，情况就有所不同了：</p><pre class=" language-she"><code class="language-she">command_does_not_exist 2>&1 > outfilecommand_does_not_exist > outfile 2>&1</code></pre><p>这两个命令看似没什么区别，但实际上第一行会把错误信息输出终端，而第二个输出错误，而是把错误信息存进了<code>outfile</code>中。区别就在于，第一行首先把标准错误流的输出指向了标准输出流指向的地方——终端，之后它再把标准输出流重定向到了文件。而第二行首先把<code>command_does_not_exist</code> 的标准输出流重定向到了<code>outfile</code>，之后再把标准错误流指向标准输出流指向的地方，而这时，是文件。</p><p>管道则是一个非常形象的名字。它将前一个命令的标准输出于是弄给下一个命令作为输入，好像通过管道连接一样。管道运算符是<code>|</code>：</p><pre class=" language-shell"><code class="language-shell">cat file1 | grep -c file</code></pre><p>这个命令会找出文件<code>file1</code> 中含有“file”的行数。</p><p>但管道是只接收标准输出流的。假如上面这条命令中，<code>file1</code>这个文件并不存在， 则会输出以下结果：</p><p><img src="https://s2.ax1x.com/2019/11/23/MbeuLR.md.png" alt="MbeuLR.md.png"></p><p>可见标准错误流并不能通过管道。但如果你把上面的 <code>|</code> 换成 <code>|&amp;</code>结果就完全不同了：<code>|&amp;</code> 可以同时传送标准输入和标准错误，因为这行错误信息中有一个“file”，因此输出结果会是1。</p><p>管道和重定向有相似之处，管道传送的对象是另一个命令，而重定向的对象则是文件。</p><h2 id="Step-4"><a href="#Step-4" class="headerlink" title="Step 4"></a>Step 4</h2><h3 id="Test-and-if-statement：测试和条件"><a href="#Test-and-if-statement：测试和条件" class="headerlink" title="Test and if statement：测试和条件"></a>Test and if statement：测试和条件</h3><p>test允许我们实现条件表达式。我们在一对方括号中放入被测试的条件，或者之前在前方放<code>test</code>:</p><pre class=" language-bash"><code class="language-bash"><span class="token punctuation">[</span> 1 <span class="token operator">=</span> 0 <span class="token punctuation">]</span><span class="token function">test</span> 1 <span class="token operator">=</span> 0</code></pre><p>可以用<code>echo $?</code>来获得上一条指令的结果。与其他编程语言不同，在bash中，假值会返回一个非零值，而真值会返回0。这点需要格外注意。</p><p>不过bash也提供了和很多编程语言相同的逻辑运算符。如：</p><pre class=" language-bash"><code class="language-bash"><span class="token punctuation">(</span> <span class="token punctuation">[</span> 1 <span class="token operator">=</span> 1 <span class="token punctuation">]</span> <span class="token operator">||</span> <span class="token punctuation">[</span> <span class="token operator">!</span> <span class="token string">'0'</span> <span class="token operator">=</span> <span class="token string">'0'</span> <span class="token punctuation">]</span> <span class="token punctuation">)</span> <span class="token operator">&amp;&amp;</span> <span class="token punctuation">[</span> <span class="token string">'2'</span> <span class="token operator">=</span> <span class="token string">'2'</span> <span class="token punctuation">]</span><span class="token punctuation">[</span>  1 <span class="token operator">=</span> 1 -o  <span class="token operator">!</span> <span class="token string">'0'</span> <span class="token operator">=</span> <span class="token string">'0'</span>  -a  <span class="token string">'2'</span> <span class="token operator">=</span> <span class="token string">'2'</span> <span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># -o: or, -a: and, but you cannot use () to group within them.</span></code></pre><p><code>[[</code> 和 <code>[</code>很类似，实际上在大多数情况下他们表现得别无二致，区别在于前者对不存在的变量名的处理方式。如下所示：</p><pre class=" language-bash"><code class="language-bash">unset DOESNOTEXIST<span class="token punctuation">[</span> <span class="token variable">${DOESNOTEXIST}</span> <span class="token operator">=</span> <span class="token string">''</span> <span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># treat it as [ = '' ], so bash will complain</span><span class="token keyword">echo</span> <span class="token variable">$?</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token variable">${DOESNOTEXIST}</span> <span class="token operator">=</span> <span class="token string">''</span> <span class="token punctuation">]</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># treat it as [[ '' = '' ]]</span><span class="token keyword">echo</span> <span class="token variable">$?</span><span class="token punctuation">[</span> x<span class="token variable">${DOESNOTEXIST}</span> <span class="token operator">=</span> x <span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># treat it as [ x = x ]</span><span class="token keyword">echo</span> <span class="token variable">$?</span></code></pre><p>下面介绍一些常见的操作符，包含一元和二元操作符。</p><p>常用的一元运算符如下：</p><pre class=" language-bash"><code class="language-bash"><span class="token function">touch</span> tests_file   <span class="token comment" spellcheck="true"># Create tests file</span><span class="token function">mkdir</span> tests_dir<span class="token punctuation">[</span> -a tests_file <span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># -a returns true if file exists</span><span class="token keyword">echo</span> <span class="token variable">$?</span><span class="token punctuation">[</span> -d tests_file <span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># -d returns false if the directory does not exist</span><span class="token keyword">echo</span> <span class="token variable">$?</span>            <span class="token comment" spellcheck="true"># A normal file is not a directory</span><span class="token punctuation">[</span> -a tests_dir <span class="token punctuation">]</span>   <span class="token comment" spellcheck="true"># A directory is a type of file, so returns true</span><span class="token keyword">echo</span> <span class="token variable">$?</span><span class="token punctuation">[</span> -d tests_dir <span class="token punctuation">]</span>   <span class="token comment" spellcheck="true"># -d returns true if directory exists</span><span class="token keyword">echo</span> <span class="token variable">$?</span><span class="token punctuation">[</span> -z <span class="token string">"<span class="token variable">$PWD</span>"</span> <span class="token punctuation">]</span>      <span class="token comment" spellcheck="true"># -z is a unary operator that takes one argument</span><span class="token keyword">echo</span> <span class="token variable">$?</span>            <span class="token comment" spellcheck="true"># Returns false, as PWD has content</span></code></pre><p>常见的二元运算符主要是比较操作符。但需要注意bash中<code>&lt;</code> <code>&gt;</code> <code>=</code>是定义在字符串上的操作符，在整数上应该使用<code>-lt</code> <code>-gt</code> <code>-eq</code>。下例展示了这几个操作符的用法，顺便也介绍了<code>if</code> statement的写法：</p><pre class=" language-bash"><code class="language-bash"><span class="token keyword">if</span> <span class="token punctuation">[</span><span class="token punctuation">[</span> 10 -lt 2 <span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token keyword">then</span>  <span class="token keyword">echo</span> <span class="token string">'does not compute'</span><span class="token keyword">elif</span> <span class="token punctuation">[</span><span class="token punctuation">[</span> 10 -gt 2 <span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token keyword">then</span>  <span class="token keyword">echo</span> <span class="token string">'computes'</span><span class="token keyword">else</span>  <span class="token keyword">echo</span> <span class="token string">'neither greater than, or less than, so must be equal'</span><span class="token keyword">fi</span></code></pre><h3 id="Loops：循环"><a href="#Loops：循环" class="headerlink" title="Loops：循环"></a>Loops：循环</h3><p>因为循环语句在各种编程语言中非常常见，此处仅以例子说明。</p><p>for循环：</p><pre class=" language-bash"><code class="language-bash"><span class="token keyword">for</span> <span class="token variable"><span class="token punctuation">((</span> i<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> <span class="token number">20</span><span class="token punctuation">;</span> i<span class="token operator">++</span> <span class="token punctuation">))</span></span><span class="token keyword">do</span>  <span class="token keyword">echo</span> <span class="token variable">$i</span>  <span class="token keyword">echo</span> <span class="token variable">$i</span> <span class="token operator">></span> <span class="token function">file</span><span class="token variable">${i}</span>.txt<span class="token keyword">done</span></code></pre><pre class=" language-bash"><code class="language-bash"><span class="token keyword">for</span> f <span class="token keyword">in</span> <span class="token variable"><span class="token variable">$(</span><span class="token function">ls</span> *txt<span class="token variable">)</span></span><span class="token keyword">do</span>    <span class="token keyword">echo</span> <span class="token string">"File <span class="token variable">$f</span> contains: <span class="token variable"><span class="token variable">$(</span><span class="token function">cat</span> $f<span class="token variable">)</span></span>"</span><span class="token keyword">done</span></code></pre><p>while循环：</p><pre class=" language-bash"><code class="language-bash">n<span class="token operator">=</span>0<span class="token keyword">while</span> <span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token operator">!</span> -a newfile <span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token keyword">do</span>    <span class="token variable"><span class="token punctuation">((</span>n<span class="token operator">++</span><span class="token punctuation">))</span></span>    <span class="token keyword">echo</span> <span class="token string">"In iteration <span class="token variable">$n</span>"</span>    <span class="token keyword">if</span> <span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token variable"><span class="token variable">$(</span><span class="token function">cat</span> file$<span class="token punctuation">{</span>n<span class="token punctuation">}</span>.txt<span class="token variable">)</span></span> <span class="token operator">==</span> <span class="token string">"15"</span> <span class="token punctuation">]</span><span class="token punctuation">]</span>    <span class="token keyword">then</span>            <span class="token function">touch</span> newfile    <span class="token keyword">fi</span><span class="token keyword">done</span></code></pre><p>到这里，我认为简单的Bash tutorial就暂时结束了。虽然没有涉及很多具体的命令，但我们建立了一个Bash的big picture。当然这里缺少了最关键的部分：shell脚本。不过我希望能有空再写一篇博客谈谈这个。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;对程序员而言，Shell是一种必备技能。目前，Bash是最流行的Shell程序之一，因此学会Bash的使用是很有必要的。&lt;/p&gt;
&lt;h2 id=&quot;Step-01&quot;&gt;&lt;a href=&quot;#Step-01&quot; class=&quot;headerlink&quot; title=&quot;Step 01&quot;&gt;&lt;</summary>
      
    
    
    
    <category term="tech" scheme="https://huangweiran.club/categories/tech/"/>
    
    
    <category term="shell" scheme="https://huangweiran.club/tags/shell/"/>
    
  </entry>
  
  <entry>
    <title>Tricks of training neural nets</title>
    <link href="https://huangweiran.club/2019/10/04/Tricks-of-training-neural-nets/"/>
    <id>https://huangweiran.club/2019/10/04/Tricks-of-training-neural-nets/</id>
    <published>2019-10-05T03:22:18.000Z</published>
    <updated>2019-10-06T07:31:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>How to train neural networks has been a huge challenge to researchers since the very first day of deep learning. <a id="more"></a>Unlike most classic machine learning algorithms, neural networks, especially deep ones, require more data and more parameters to tune, which make them more versatile, powerful and scalable to express the distribution of  real-world data, but also create huge difficulty for training.</p><p>Neural networks, or artificial neural networks(ANNs), are said to be inspired by the biological neurons. It was first introduced in 1943 (surprisingly early, uh?), but progress was slow.  The training technique remained challenging until the invention of back propagation in 1986. In short, it is still gradient descent as in many classic machine learning algorithms, but back propagation combines it with chain rule. Back propagation is an algorithm that is still widely used today, and it is with no doubt one of the biggest breakthroughs in the history of deep learning. With this algorithm, for the first time people began to believe that it is feasible to train a neural network. However, due to the complex hypothesis space, training a neural net still requires much experiences and some fancy tricks, and this is why I am writing this blog post today.</p><p>Most part of this article comes from the wonderful book, <em>Hands on Machine Learning with Scikit-learn and Tensorflow, 2nd edition</em>, many thanks to the authors.</p><h2 id="Basics"><a href="#Basics" class="headerlink" title="Basics"></a>Basics</h2><h3 id="Tools-for-optimization"><a href="#Tools-for-optimization" class="headerlink" title="Tools for optimization"></a>Tools for optimization</h3><p>Grid search and randomized search have been universal solutions to fine-tuning the hyperparamters for most machine learning algorithms, and luckily, they can be applied to deep learning too. If you are using Keras or tf.Keras, you are super lucky because you can wrap up your Keras models with <code>keras.wrappers.scikit_learn</code>, so that you can use <code>GridSearchCV</code> and <code>RandomizedSearchCV</code> in scikit-learn. </p><p>You can also use some Python libraries to optimize hyperparameters, for example, Hyperopt, hyperas, Scikit-Optimize. Also, companies like Google are offering hyperparameter optimization services now, such as hyperparameter tuning services provided by Google Cloud ML Engine. Moreover, nowadays, AutoML has been a heated topic, and there have been such kind of services available: maybe the handmade training era will end soon?</p><p>It is possible, and very promising. But we have to admit there is still a long way to go, and that is why you still need to learn about training tricks today.</p><h3 id="Tips-for-tuning-hyperparamters"><a href="#Tips-for-tuning-hyperparamters" class="headerlink" title="Tips for tuning hyperparamters"></a>Tips for tuning hyperparamters</h3><p>We all know deeper networks can model more complex functions, but they are harder to train. So when we are designing our models, it can be tough to decide <strong>the number of hidden layers</strong>. For this, the best (and maybe easiest) practice could be gradually ramp up the numbers until starting to overfitting.</p><p>Another hyperparameter you have to consider is <strong>the number of neurons per layer</strong>. People found that using the same number of neurons in all hidden layers performs good enough. It is good because in this way we only have one hyperparameter to tune instead of one per layer. Just like for the number of layers, we can try increasing the number of neurons gradually until the network starts overfitting. But experience tells us it is hard to implement. A simpler approach is to pick a model with more layers and neurons than we actually need, and then use <strong>early stopping</strong> and other regularization techniques such as <strong>dropout</strong> to prevent our nets from overfitting.</p><p>Other hyperparameters includes the <strong>learning rate, batch size</strong> and so on. As for the <strong>learning rate</strong>, in general, the optimal learning rate is about half of the maximum learning rate. So we can start with a large learning rate that will make our training algorithm diverge, and then divide the value by 3 and try again, and repeat until the training algorithm stops diverging. The optimal <strong>batch size</strong> is usually lower than 32. We know larger batch size can lead to more precise estimate of the gradients, but since we normally will not use the vanilla gradient descent, the precision of gradients are not that important than for other machine learning algorithms. On the other hand, however, we know, having a larger batch means higher degree of parallelism and therefore, faster training. So normally the batch size will not exceed 32, but will also not be too small. Note that if you use batchNorm, the size should generally larger than 20. Further, we don’t really need to tweak the number of training iterations, because <strong>early stopping</strong> is preferred.</p><h2 id="Advanced-Topics"><a href="#Advanced-Topics" class="headerlink" title="Advanced Topics"></a>Advanced Topics</h2><h3 id="Activation-functions"><a href="#Activation-functions" class="headerlink" title="Activation functions"></a>Activation functions</h3><p><strong>Gradient vanishing</strong> is a long-existing problem when training deep neural nets. It refers to the phenomenon that the gradients get smaller and smaller as back propagation progresses down to the lower layers. When gradient vanishing happens, the weights of lower layers remains hardly changed because their gradients are neglectable.</p><p>In 2010, a paper written by Xavier Glorot and Yoshua Bengio(one of the Turing Award receiver in 2018) dived deep in this problem, and found a few suspects. One of these suspects is the sigmoid activation function and the normal distribution initialization scheme. They found that under this setting, the variance of the outputs is much greater than the inputs and will keeps increasing until the sigmoid function saturates, i.e. gets close to the value 0 or 1. When it saturates, the gradients are close to zero so there is virtually no gradient to propagate back through the network, then gradient vanishing happens.</p><p>Glorot and Bengio argue that to solve this problem, we need <strong>the variance of the outputs of  outputs of each layer to be equal to the variance of its input, we also need the gradients to have equal variance before and after flowing through a layer in the reverse direction</strong>. They further proposed a novel initialization method, called Xavier initialization or Glorot initialization. The pseudo-code is as below:</p><pre class=" language-pseudocode"><code class="language-pseudocode">for-each input-hidden weight  variance = 2.0 / (fan-in +fan-out)  stddev = sqrt(variance)  weight = gaussian(mean=0.0, stddev)end-for</code></pre>As you can see, it is just a normal distribution with mean 0 and variance $\sigma^2 = \dfrac{2}{\text{fan}_{in} + \text{fan}_{out}}$There is also a uniform distribution version: the range of this uniform distribution is from $-\sqrt{\dfrac{6}{(\text{fan}_{in} + \text{fan}_{out})}}$  to $\sqrt{\dfrac{6}{(\text{fan}_{in} + \text{fan}_{out})}}$.Glorot initialization proves good for activation functions like linear(i.e. no activation), tanh, sigmoid and softmax. There are also some variants, whose difference is only the scale of the variance and whether they use $\text{fan}_{in}$ or $\text{fan}_{avg} = \dfrac{\text{fan}_{in} + \text{fan}_{out}}{2}$. For example, He initialization is suitable for ReLU and its variants like ELU, and its variance is $\dfrac{2}{\text{fan}_{in}}$. LeCun initialization differs from Glorot only in that it uses $\text{fan}_{in}$ instead of $\text{fan}_{avg}$, and it is for SELU.All these methods have uniform version, you can get the range $r$ by the equation: $r =\sqrt{3\sigma^2}$. <p>Keras uses Glorot initialization with a uniform distribution by default.</p><p>Because of Glorot and Bengio, people started to realize that ReLU is a better activation function than sigmoid in deep learning, for the positive values will not saturate and it is quite fast to compute. But ReLU also has its own drawbacks.  The problem is called <em>dying ReLUs</em>: during training, some neurons stop output anything but 0, so they are effectively dead. To solve this, people came up with some variants of ReLU, for example, the leaky ReLU = $\max(\alpha z,z)$ and the randomized leaky ReLU(RReLU). The parametric leaky ReLU treats $\alpha$ as a learnable parameter. Experiments show it strongly outperforms ReLU on large image datasets but tends to overfit on smaller datasets.</p><p>In 2015, a new activation function called <em>exponential linear unit</em>(ELU) was proposed. It is reported to outperform all ReLU variants both in training time and the results of the test set. The only difference between ELU and leaky ReLU is that its negative part is $\alpha(\exp(z) - 1)$ instead of $\alpha z$. Notice that the exponential function makes it slower at test time. SELU is just a scaled version of ELU.</p><p>From practice, we say in general SELU &gt; ELU &gt; leaky ReLU (and its variants) &gt; ReLU &gt; tanh &gt; sigmoid.</p><h3 id="Optimizers"><a href="#Optimizers" class="headerlink" title="Optimizers"></a>Optimizers</h3><p>Since we are talking about <em>tricks</em> here, the algorithm details of each optimizer will not be emphasized. For more details, refer to the book I mentioned above, it has a wonderful introduction to these optimizers.</p><ul><li>Tl;dr, use Adam. But if you found your dataset may just be allergic to adaptive gradients, try plain NAG.</li><li>When using momentum, the momentum value of 0.9 usually works well.</li><li>Nesterov Accelerated Gradient(NAG) is a small variant of momentum, and <strong>is always faster than vanilla Momentum optimization</strong>.</li><li>Adagrad often stops too early when training neural networks.</li><li>RMSProp fixes the problem of Adagrad by accumulating only the gradients from the most recent iterations, the decay rate is typically set to 0.9 and it almost works well for all cases so you don’t have to tune it. Except on very simple problems, it almost always beats Adagrad.</li><li>Adam combines the idea of momentum and RMSProp.  The momentum decay hyperparameter is typically set to 0.9, while the scaling decay hyperparameter is often initialized to 0.999. <strong>Since Adam is an adaptive learning rate algorithm, it requires less tuning of the learning rate</strong>.</li></ul><h3 id="Batch-normalization"><a href="#Batch-normalization" class="headerlink" title="Batch normalization"></a>Batch normalization</h3><p>In a 2015 paper, Sergey Ioffe and Christian Szegedy proposed a technique called <em>Batch Normalization (BN)</em> to address the vanishing/exploding gradients problems.</p><p>The technique consists of adding an operation in the model just before or after the activation function of each hidden layer, simply zero-centering and normalizing each input, then scaling and shifting the result using two new parameter vectors per layer: one for scaling, the other for shifting. In other words, this operation <strong>lets the model<br>learn the optimal scale and mean of each of the layer’s inputs</strong>. </p><p>This method is thoroughly discussed all over the internet, and the steps are quite clear and simple, so I will not talk about the algorithm details here.  </p><p>The authors claimed that this technique considerably improved <strong>all the DNNs</strong> they experimented with. The vanishing gradients problem was strongly reduced and the networks become less sensitive to the weight initialization. They were able to use large learning rates to speed the training process.</p><h3 id="Gradient-clipping"><a href="#Gradient-clipping" class="headerlink" title="Gradient clipping"></a>Gradient clipping</h3><p>Though it achieved very good performance, BatchNorm is tricky to use in RNNs. One method to deal with <strong>gradient exploding</strong> is gradient clipping. </p><p>In Keras, implementing gradient clipping can be achieved by just setting the <code>clipvalue</code> or <code>clipnorm</code> argument when creating an optimizer:</p><pre class=" language-python"><code class="language-python">optimizer <span class="token operator">=</span> keras<span class="token punctuation">.</span>optimizer<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>clipvalue<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span>model<span class="token punctuation">.</span>compile<span class="token punctuation">(</span>lose<span class="token operator">=</span><span class="token string">"mse"</span><span class="token punctuation">,</span> optimizer<span class="token operator">=</span>optimzer<span class="token punctuation">)</span></code></pre><p>With the above code,  every component of the gradient vector will be clipped to a value between -1.0 and 1.0. In practice it works quite good, but you may argue that by doing this, the direction of the gradient vector will be changed drastically under this setting. For example, if the gradient vector is [0.9, 100], after clipping you will get [0.9, 1.0]. To avoid this, you may try the argument <code>clipnorm</code>, <strong>this will clip the whole gradient if its L2 norm is greater than the threshold</strong>. Like we said, simply performing clipping by value is good enough in most cases. In practice you may try both and see the results on the validation set to decide.</p><h2 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h2><p>The book actually covers much more than what I mentioned above, for example, reusing pretrained layers, dropout and MC dropout. You should visit chapter 10 and chapter 11 if you need more guidelines.</p><p>I will also actively add other topics into this article whenever I found some tricks useful.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;How to train neural networks has been a huge challenge to researchers since the very first day of deep learning.</summary>
    
    
    
    <category term="ML" scheme="https://huangweiran.club/categories/ML/"/>
    
    
    <category term="tutorial" scheme="https://huangweiran.club/tags/tutorial/"/>
    
    <category term="Deep Learning" scheme="https://huangweiran.club/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning Topics</title>
    <link href="https://huangweiran.club/2019/06/07/Machine-Learning-Topics/"/>
    <id>https://huangweiran.club/2019/06/07/Machine-Learning-Topics/</id>
    <published>2019-06-08T03:38:35.000Z</published>
    <updated>2019-07-24T05:33:54.000Z</updated>
    
    <content type="html"><![CDATA[<p>接触机器学习快两年的工夫，时常会深感自己理论基础的薄弱。但在实验室一年，更多的时候是在接触更前沿的技术，而无暇去补足基础知识的缺失和零散，导致现在知识难成体系。这对我个人的发展是一个不可忽视的问题。</p><a id="more"></a><p>最近越来越觉得，学习一个学科，了解一个领域，最重要的是要有<strong>大局观（Big picture）</strong>。在选择某个子领域去精进之前，你必须对整个领域有一个大致的把握。如今各个领域的交叉性日益显著，只守着自己的一亩三分地，很容易把自己的思维限制住。这可能也是为什么有人会说学深度学习的人也不能忽视传统机器学习，做NLP的人也得关注CV。</p><p>因此我想在这个夏天把我的知识体系化。目前我开始复习一些知识，整理一些笔记，希望能在美国这几个月的清闲时间里完成这项工作。</p><p>目前已完成的有：</p><ul><li>2019/07/24：<a href="https://github.com/EtoDemerzel0427/Misc-Notes/blob/master/Machine%20Learning%20Notes/Decision%20Trees.pdf" target="_blank" rel="noopener">Decision Trees</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;接触机器学习快两年的工夫，时常会深感自己理论基础的薄弱。但在实验室一年，更多的时候是在接触更前沿的技术，而无暇去补足基础知识的缺失和零散，导致现在知识难成体系。这对我个人的发展是一个不可忽视的问题。&lt;/p&gt;</summary>
    
    
    
    <category term="ML" scheme="https://huangweiran.club/categories/ML/"/>
    
    
    <category term="ML" scheme="https://huangweiran.club/tags/ML/"/>
    
    <category term="Scikit-learn" scheme="https://huangweiran.club/tags/Scikit-learn/"/>
    
    <category term="Tensorflow" scheme="https://huangweiran.club/tags/Tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>《北鸢》：一次野心超越了能力的尝试</title>
    <link href="https://huangweiran.club/2018/11/16/%E3%80%8A%E5%8C%97%E9%B8%A2%E3%80%8B%EF%BC%9A%E4%B8%80%E6%AC%A1%E9%87%8E%E5%BF%83%E8%B6%85%E8%B6%8A%E4%BA%86%E8%83%BD%E5%8A%9B%E7%9A%84%E5%B0%9D%E8%AF%95/"/>
    <id>https://huangweiran.club/2018/11/16/%E3%80%8A%E5%8C%97%E9%B8%A2%E3%80%8B%EF%BC%9A%E4%B8%80%E6%AC%A1%E9%87%8E%E5%BF%83%E8%B6%85%E8%B6%8A%E4%BA%86%E8%83%BD%E5%8A%9B%E7%9A%84%E5%B0%9D%E8%AF%95/</id>
    <published>2018-11-17T04:38:02.000Z</published>
    <updated>2018-11-17T04:38:02.000Z</updated>
    
    <content type="html"><![CDATA[<p>这是一本你看了会觉得“它本可以写的更好”的小说，或者至少我这么觉得。</p><a id="more"></a><h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>这是一篇读书笔记，希望在2018年我能坚持这个习惯。内容只含个人感受，因本人鉴赏水平有限，不具有参考价值，具体评判需由读者自己定夺。</p><p><strong>Update</strong>： 我觉得我有必要给这篇所谓“书评”增加一点内容，更新于2018年1月24日。</p><h2 id="小说简介"><a href="#小说简介" class="headerlink" title="小说简介"></a>小说简介</h2><p>以下简介摘自豆瓣。</p><p><img src="https://s1.ax1x.com/2018/09/03/PzMMPP.jpg" alt="s29051355.jpg-9.7kB"><br>内容简介：</p><blockquote><p>《北鸢》起笔于民国商贾世家子弟卢文笙的成长，收束于上世纪中叶。将波诡云谲的民国动荡史寄予两个家族的命运沉浮，书写中国最为丰盛起伏的断代。人生一线，恰似风筝。命运漂浮无着，人亦应有自己的主心骨。政客、军阀、寓公、文人、商人、伶人，书中上百位经典民国人物，进退于沧桑。群落交织，浑然磅礴。文笙在大时代的风云中辗转历练，且行且进，最终尘埃落定。每个人都有自己的来处，了解来处，才知道自己的去向。在故事的日常精微与家族兴颓里，藏着我们过去以及未来的影子。</p></blockquote><p>作者简介：</p><blockquote><p>葛亮，原籍南京，现居香港，任教于高校。香港大学中文系博士。作品出版于两岸三地，著有小说《北鸢》《朱雀》《七声》《谜鸦》《浣熊》《戏年》，文化随笔《绘色》，学术论著《此心安处亦吾乡》等。部分作品译为英、法、俄、日、韩等国文字。<br>曾获首届香港书奖、香港艺术发展奖、台湾联合文学小说奖首奖、台湾梁实秋文学奖等奖项。作品被收入“当代小说家书系”“二十一世纪中国文学大系”，入选2008、2009、2015年“中国小说排行榜”和“2015年度诚品中文选书”。长篇小说《朱雀》获选“《亚洲周刊》全球华文十大小说”。2016年以新作《北鸢》再获此荣誉。</p></blockquote><h2 id="读书笔记"><a href="#读书笔记" class="headerlink" title="读书笔记"></a>读书笔记</h2><p>我个人非常喜欢以个人命运展现家族兴衰的小说，因此当我看到《北鸢》这本书的简介时，我毫无悬念地被它所吸引了。但看完之后，我并不觉得自己读完了一部好作品。因为，如我的标题所言，我认为这是一次<strong>野心大于能力的尝试</strong>。</p><h3 id="一-历史"><a href="#一-历史" class="headerlink" title="一. 历史"></a>一. 历史</h3><p>从简介上看，作者是在有意识地将所谓民国动荡史加入小说内容的，事实也的确如此。</p><p>如果安排得当，小说的确会有一种历史的真实感和厚重感。在文章的这方面的力度控制得还算稳定，只略略提到李可染是吴清舫的弟子，这样虚实交叉的内容使人会心一笑。但从昭和携文笙远赴天津开始，作者对史实和小说的掌控开始有些失控了。</p><p>小说很大胆地将褚玉璞（书中化名为石玉璞）写成昭德的丈夫，这本是一桩很有趣的事情。褚玉璞这个人物在历史上本身就很有传奇色彩，他出身绿林，在革命浪潮中却成为一代军阀，但最终却惨遭活埋。这个角色如果作者深入刻画，相信能有不错的效果。但小说中对他的处理，给人的感觉是：<strong>作者完全把他当作一个历史人物，而非一个小说角色在处理</strong>。因此这个人物在小说中异常单薄的，他性情暴戾无常，但也仅此而已了。不过另外值得一提的是，褚玉璞枪杀五姨太也是历史上真实发生的故事。小说中对五姨太晓琴的角色刻画倒显得不错。你能感受到一个出身贫寒被迫嫁给军阀的年轻女孩的可爱和对真正爱情的向往。</p><p> 而当情节进入抗日战争的部分之后，作者掌控力的单薄已经显露无遗。我们这一代人接触了非常多关于这一时期的文学作品和影视作品，因此你非常容易发现葛亮的这部作品里，关于抗日战争的部分是非常<strong>不接地气</strong>的，以至于写出了一些传奇色彩，像极了网络上那些不入流的民国小说。在避难阶段，作者将教堂环境描写地宛如世外桃源，就已经给人强烈的割裂感了；更不必说那几个中文流利、在救治伤员之余还教中国孩子学英文的外国神职人员了。至于那位慰安妇小蝶舍身杀敌的剧情，我只能说，狗血得有些像我们经常见到的抗日神剧。</p><h3 id="二-人物"><a href="#二-人物" class="headerlink" title="二. 人物"></a>二. 人物</h3><p>再说到人物刻画方面。前面我已经提到石玉璞这个角色，但小说的问题远远不止这一个人物。在我看来，《北鸢》中大部分人物的性格都是单薄的，有时甚至是单一的，很多人物性格给人很相似的感觉——这一点尤其体现在配角上。而一些重要的配角，如凌佐、尹小姐、言秋凰等，我们可以看出作者的确下了些功夫，使得这些人物有些自己的特色，在一定程度上令人印象深刻。但这些角色总给人一种符号化的印象，在短篇小说中也许会很有特色，但在这样的长篇中，就略显俗套——长篇小说的角色应该更丰满些。</p><p>孟昭如这个角色我想单独说说。书的前半部分里，她是一个开明、知书达礼、心地善良的角色，能当众收养弃儿（这在封建文化里当然是很难得的），能接纳继女，遇人亲和，对待身边的仆从都照料周到。但最后，却莫名其妙成为了一个反对儿子参军事业、通过幽禁儿子来阻止其自由恋爱的迂腐角色。我以为，这是一个人物刻画上的矛盾。</p><p>而两位主角的刻画就更令人失望了。看官们或许会记得，小说开篇卢文笙抓阄未选一物的情节，吴清舫当时称赞说这是说明这孩子“不为世事所动”。看到这里的时候，每个读者想必都会认为这处情节应该是为卢文笙一生定下基调。但读罢全文，你很难将此与卢文笙这个角色联系起来；事实上，你（也许只是我）甚至很难对他的性格有所把握。他这一生上过学，逃过难，参过军，但参军被家人押回后，竟也就安心留在家中恋爱，而后又随人前往上海经商。作为小说的主角，他身上没有经历过什么性格上的转变，性格也没有明显的特色。而冯文桢，在小说的前半部分，基本是一个看客的身份出场的。她的存在意义仿佛是给读者提供一个观察冯家的视角，发生的情节大部分也与她没有关系，因此很难说在这部分内容里她能有什么所谓性格。在这部分，她因为革命丧亲，因此对革命一类的事情相当反感，而后半部分，抗战胜利后，她去到杭州大学，反而成为一个积极参与民主活动的女大学生；其中是何变故，也让人摸不着头脑。作者称这两个主角是<strong>“一静一动”</strong>，但恕我直言，这很难与他实际描写出来的角色契合。</p><p>顺便一提，《北鸢》这部小说中唯一令我觉得刻画得不错的人物是冯仁珏和冯明焕，前者有血有肉，后者虽然着笔不多，但特点鲜明。</p><h3 id="三-情节"><a href="#三-情节" class="headerlink" title="三. 情节"></a>三. 情节</h3><p>小说中出现的两个家庭，卢家和冯家，在大部分时间里是割裂的关系，像两个并行的故事一样。作者力图通过文笙和仁桢串起两个家族的关系，反倒显得有些莫名其妙。这两人从年少时的互相吸引，到长大后的恋爱，都让人觉得有些牵强。小说中甚至找不到一处关于他们两人爱情发展的关键情节。对比《金粉世家》，作者在此处的着力显得太不够了。可以说，无论是简单的二人恋爱，还是恋爱背后的家族关系，作者都没能写好。</p><p>另外一点，这两个家族都是在小说开篇就已经处在没落的状态，没有经历由繁盛到衰败的阶段，导致作者一直在渲染这种<strong>凉薄凄清</strong>的氛围，乃至用力过猛；因此简介中的“将波诡云谲的民国动荡史寄予两个家族的命运沉浮”这顶高帽子并不能戴给《北鸢》。</p><p>在细节上的处理，我也对它有些不满意。比如：凌佐托付给文笙安葬的继父的“命根子”，最后如何处理的？文中没有交代。六太太荣芝曾刺探过文笙的来历，为此甚至逼走过一个女仆，最后查出什么没有？文中也没有再提。后者令我颇为遗憾，因为这本是能生发出不错的剧情的一个伏笔：如果查出来了六太太会在家中激起怎样的戏剧冲突？文笙知道真相后，会如何自处？但这个本该成为伏笔的内容最后却成为了一个“废笔”，也导致“文笙被收养”的这个剧情，在情节上毫无作用。</p><h3 id="四-总述"><a href="#四-总述" class="headerlink" title="四. 总述"></a>四. 总述</h3><p>据说作者为写此书，花了七年的时间进行相关考据；但他似乎没有抓住小说真正重要的内容。他没能把握好“民国”这个时代背景（或者说他对这个时代背景过于痴迷，以至于一直在刻意渲染所谓的“民国氛围“），也没能写好“家族”这个庞大的主题。</p><p>如果要我给这本书打分的话，以五星为满分，我会给它两星半。</p><p>谨此为记。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;这是一本你看了会觉得“它本可以写的更好”的小说，或者至少我这么觉得。&lt;/p&gt;</summary>
    
    
    
    <category term="Literature" scheme="https://huangweiran.club/categories/Literature/"/>
    
    
    <category term="book review" scheme="https://huangweiran.club/tags/book-review/"/>
    
    <category term="novels" scheme="https://huangweiran.club/tags/novels/"/>
    
  </entry>
  
  <entry>
    <title>Introduction to Markov chain</title>
    <link href="https://huangweiran.club/2018/09/02/Introduction%20to%20Markov%20chain/"/>
    <id>https://huangweiran.club/2018/09/02/Introduction%20to%20Markov%20chain/</id>
    <published>2018-09-03T04:36:58.000Z</published>
    <updated>2018-09-03T04:36:58.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>马尔可夫链（Markov chain）</strong>是数学建模和机器学习常用的工具（据说尤其在<strong>NLP</strong>中，我目前尚不了解很多，但之前曾看过一篇<a href="https://codeburst.io/how-i-generated-inspirational-quotes-with-less-than-20-lines-of-code-38273623c905" target="_blank" rel="noopener">用简单的马尔可夫链实现一个<strong>鸡汤生成器</strong>的博文</a>，有兴趣的朋友可以看看）。这篇文章将对它做一个简单的介绍。</p><p>以下内容为本人在参考了一些资料后的原创，因此版权属于本人。<strong>欢迎转载，但请标明原作者和原链接。</strong> </p><p>由于内容比较繁多，我将在未来一段时间内完成这篇文章。</p><p><strong>另注</strong>：根据作者测试，本文在移动端存在一个问题：<strong>公式无法显示完全</strong>。 解决办法是点击公式，使其出现选择框；长按至出现选项；选择<code>Math Settings</code> 里的 <code>Scale All Math...</code> 将scale调为大概50%，即可显示完全。</p><p>如下图所示：</p><p><img src="http://static.zybuluo.com/EtoDemerzel/3130bv3lmtaef0vw98loxjw7/image_1c52vkbaacsv1euj1eqabv61b2rm.png" alt="image_1c52vkbaacsv1euj1eqabv61b2rm.png-67.5kB"></p><h2 id="什么是Markov-chain？"><a href="#什么是Markov-chain？" class="headerlink" title="什么是Markov chain？"></a>什么是Markov chain？</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>维基百科上给出的定义如下：</p><blockquote><p>马尔可夫链（英语：<strong>Markov chain</strong>），又称离散时间马尔可夫链（<strong>discrete-time Markov chain</strong>，缩写为<strong>DTMC</strong>），因俄国数学家安德烈·马尔可夫（俄语：<strong>Андрей Андреевич Марков</strong>）得名，为状态空间中经过从一个状态到另一个状态的转换的随机过程。该过程要求具备<strong>“无记忆”</strong>的性质：下一状态的概率分布<strong>只能由当前状态决定</strong>，在时间序列中它前面的事件均与之无关。这种特定类型的“无记忆性”称作<strong>马尔可夫性质</strong>。马尔科夫链作为实际过程的统计模型具有许多应用。</p></blockquote><p>而用形式化的语言描述则为：<br>当等式两边的条件概率都有意义时，<br>$$P(X_{n+m} = j\space| X_{n} = i, X_{n-1} = i_{n-1},…X_{1} = i_{1} ) = P(X_{n+m} = j\space| X_{n} = i)$$<br>$m = 1$ 时等式成立，则随机变量序列 $X_n$ 是一个<strong>马尔可夫链</strong>, $X_i$ 的可能值构成的<strong>可数集</strong>称为该链的<strong>状态空间（state space）</strong>。</p><h3 id="定义的推论"><a href="#定义的推论" class="headerlink" title="定义的推论"></a>定义的推论</h3><p>使用<strong>数学归纳法</strong>容易证明， 若 $m = 1$时 上式成立，则 $m$ 为任意正整数都成立。<br>要完成这个证明，我们先证明这样一个引理：</p><blockquote><p><strong>引理1</strong> $P(A|B) = \sum P(A|B\cap C_i)\times P(C_i|B)$<br>其中，$\sum P(C_i) = 1$。</p></blockquote><p><strong>证明</strong>：</p><p> $$\sum P(A|B\cap C_i)\times P(C_i|B)=\sum \frac{P(A\cap B\cap C_i)}{P(B\cap C_i)}\times P(C_i|B)= \sum \frac{P(A\cap B\cap C_i)}{P(B)P(C_i|B)}\times P(C_i|B) \\= \sum \frac{P(A\cap B\cap C_i)}{P(B)} = \frac{\sum P(A\cap B \cap C_i)}{P(B)} = \frac{P(A\cap B)}{P(B)}= P(A|B)$$</p><p>由<strong>引理1</strong>，我们有：<br>$$P(X_{n+m+1} = k\space|X_n = i, X_{n-1} = i_{n-1},…X_1 = i_1 )\\=\sum_{k’ \in S}P(X_{n+m+1} = k\space|X_{n+m}=k’,X_n = i, X_{n-1} = i_{n-1},…X_1 = i_1)\\ \times P(X_{n+m}=k’\space|X_n = i, X_{n-1} = i_{n-1},…X_1 = i_1)$$  </p><p>这里我们已经知道 $m = 1$ 时是成立的，那么<br>$$P(X_{n+m+1} = k\space| X_{n+m} = k’, X_{n+m-1} = i_{n+m-1},…X_1 = i_1 )\\=P(X_{n+m+1} = k\space| X_{n+m} = k’)\\=P(X_{n+m+1} = k\space| X_{n+m} = k,X_{n} = i_{n})$$<br>这个等式成立是因为 $X_{n+m+1}$ 只与 $X_{n+m}$ 有关，至于我们为什么要引入 $X_{n} = i$,稍后再说。</p><p>对于乘号右边的 $P(X_{n+m}=k’\space|X_n = i, X_{n-1} = i_{n-1},…X_1 = i_1)$, 根据归纳假设，有：<br>$ P(X_{n+m}=k’\space|X_n = i, X_{n-1} = i_{n-1},…X_1 = i_1) = P(X_{n+m} = k’\space|X_n=i_n)$</p><p>于是，上面的等式可以改写为：<br>$$ P(X_{n+m+1} = k\space| X_{n+m} = k’, X_{n+m-1} = i_{n+m-1},…X_1 = i_1 )\\ =\sum_{k’\in S} P(X_{n+m+1} = k\space| X_{n+m} = k’,X_{n} = i_{n}) \times P(X_{n+m} = k’\space|X_n=i_n) $$</p><p>接下来我们证明第二个引理：</p><blockquote><p><strong>引理2</strong>： $P(A\cap B|C) = P(A|B\cap C) \times P(B|C)$</p></blockquote><p><strong>证明</strong> ：<br>$$P(A|B\cap C) \times P(B|C) = \frac{P(A\cap B\cap C)}{P(B\cap C)}\times \frac{P(B\cap C)}{P(C)} = \frac{P(A\cap B\cap C)}{P(C)} = P(A\cap B|C)$$</p><p>写到这里，刚才我们引入 $X_{n} = i$ 的目的就很显然了。我们可以将刚才的等式再次改写为：<br>$$ P(X_{n+m+1} = k\space| X_{n+m} = k’, X_{n+m-1} = i_{n+m-1},…X_1 = i_1 )\\ =\sum_{k’\in S} P(X_{n+m+1} = k\space| X_{n+m} = k’,X_{n} = i_{n}) \times P(X_{n+m} = k’\space|X_n=i_n) \\ =\sum_{k’\in S} P(X_{n+m+1} = k,X_{n+m} = k’\space|X_{n} = i_{n})\\= P(X_{n+m+1} = k\space|X_n=i_n)$$</p><p>这样，我们证明了，如果$m = 1$等式成立， 当$m$为任意正整数时，该等式都成立。</p><h2 id="概率转移矩阵"><a href="#概率转移矩阵" class="headerlink" title="概率转移矩阵"></a>概率转移矩阵</h2><h3 id="开普曼-柯尔莫哥洛夫公式"><a href="#开普曼-柯尔莫哥洛夫公式" class="headerlink" title="开普曼-柯尔莫哥洛夫公式"></a>开普曼-柯尔莫哥洛夫公式</h3><p>刚刚我们分析的正是马尔可夫链的第一个性质：<strong>马氏性</strong>。<br>接下来我们要讨论另一个性质：<strong>时齐性（time-homogeneity）</strong>。<br>时齐性是指，<strong>系统由状态 $i$ 到状态 $j$ 的转移概率只依赖于其时间间隔的长短，与起始时间无关</strong>。<br>用形式化的语言描述：<br> $$P(X_{n+m} = j\space | X_{n} = i) = P(X_{n+m+k} = j\space | X_{n+k} = i)$$</p><p>既然与起始时间 $n$ 无关， 那我们就可以将概率 $P(X_{n+m} = j\space | X_{n} = i)$ 写作  $P_{ij}(m)$ 。</p><p>需要注意的是，<strong>时齐性是我们的假设</strong>，而非能通过数学推导得出的性质。我们做出这个假设是因为它符合我们现实生活中的场景。</p><p>对于符合时齐性的马尔可夫链，我们可以定义这样一个<strong>概率转移矩阵</strong>$P(m)$：<br>$P(m)$ 是以 $m$ 步转移概率$P_{ij}(m)$为元素的矩阵（即   $(P(m))_{ij} = P_{ij}(m)$，也称为该链的 <strong>$m$ 步转移矩阵</strong>。通常记 $P(1)$ 为 $P$。</p><p>它具有以下几个性质：</p><ul><li><p>  $\forall i,j\in S, 0 < P_{ij}(m) < 1$ . </p></li><li><p>  $\displaystyle{\forall i, \sum_{j\in S} P_{ij}(m) = 1}$   换句话说， $P(m)$ 的每一行都是在 $S$ 上的一个概率分布。</p></li><li><p>$P(0)$ 是一个单位矩阵。</p></li></ul><p>以上几个性质比较显然，这里就不做更多说明。</p><p>这里要重点提到的是<strong>开普曼-柯尔莫哥洛夫公式（The Chapman-Kolmogorov Equations）</strong>：</p>$ \forall m,n, P(m+n)  = P(m)P(n)\\$亦即，$\forall m,n, P_{ij}(m+n) = \displaystyle{\sum_{k\in S}P_{ik}(m)P_{kj}(n)}$ <p> <strong>证明</strong>：由<strong>引理1</strong></p> $$\displaystyle{P_{ij}(m+n) = P(X_{m+n+1} = j \space| X_1 = i)\\ = \sum_{k\in S}P(X_{m+n+1} = j |X_{m+1} = k, X_1 = i)\times P(X_{m+1} = k\space|X_1 = i)\\=\sum_{k\in S}P_{ik}(m)P_{kj}(n)}$$<p>由此，我们能得到以下两个推论：</p><blockquote><p> <strong>推论1</strong>： $ P(n) = P(n-1)P(1) = P(n-2)P(1)^2 = …P(1)^n = P^n$</p><p> <strong>推论2</strong>： 如果我们设初始的概率分布为 $P^{(0)} $（行向量）,  那么经过了 $n$ 个步骤后的概率分布 $P^{(n)} = P^{(0)}P(n) = P^{(0)}P^n$ </p></blockquote><h3 id="极限概率分布"><a href="#极限概率分布" class="headerlink" title="极限概率分布"></a>极限概率分布</h3><p>我们知道，概率分布矩阵的每个元素都属于$[0,1]$， 那么很自然地，我们就会想知道：</p><p>对其求 $n$ 次幂后得到的 $P^n$, 是否有极限呢？</p><p>对于有限的随机序列 $X_n$, $P$ 和 $P^n$ 都是大小有限的方阵。这意味着我们也许可以用单纯的线性代数思想来解决这个问题。</p><h4 id="线性代数的角度"><a href="#线性代数的角度" class="headerlink" title="线性代数的角度"></a>线性代数的角度</h4><p>纯粹从线性代数的角度来看这个问题，我们的 $P$ 具有什么性质呢？</p><blockquote><ol><li>所有元素均为非负数。</li><li>每行元素和为 $1$。</li></ol></blockquote><p>在这样性质的基础上，下面我们证明：</p><blockquote><ol><li>$P$ 的任意一个特征值 $\lambda$ 满足 $ |\lambda| &lt;= 1$。</li><li>$P$ 有特征值 $1$， 且非重根。</li></ol></blockquote><p><strong>证明</strong>：</p><ol><li><p>令 $\lambda$ 对应的特征向量 $x = (x_1, x_2,...x_n)^T$, 设 $x_i = max\{x_1,x_2...,x_n\}$。</p><p> 由于 $Px = \lambda x$,  $(Px)_{i} = \displaystyle{\sum_{j=1}^n a_{ij}x_{j}} = \lambda x_i$</p><p> 两边同时取绝对值，有  $|\lambda| |x_i| =  |\displaystyle{\sum_{j=1}^n a_{ij}x_{j}}|\\\leqslant \displaystyle{\sum_{j=1}^n a_{ij}|x_{j}}| \leqslant \displaystyle{(\sum_{j=1}^n a_{ij})|x_i| = x_i}$ </p><p> 因此 $\lambda \leqslant 1$。</p><p> ​</p></li><li><p>很容易发现 $1$ 是一个特征值，我们只需要取 $x = (1,1,..1)^T$，就可以很容易地发现 $Px = x$。</p><p>因此我们需要证明的是 $1$ 不是重根。</p><p>采用<strong>反证法</strong>。</p><p>我们已经知道 $1$ 是一个特征值，如果他是重根的话，那么 $det(P-\lambda I)$ 中至少有两个 $(1-\lambda)$ 的因子。</p><p>对于 $det(P-\lambda I)$ ， 我们把每一列的数字累加到第 $n$ 列，可使第 $n$ 列全为 $(1-\lambda)$，这里我们可以提出第一个因子。这时行列式最右一列均为 $1$， 一个很自然的想法是，前 $n -1 $ 行减去第 $n$ 行：这样第 $n$ 列就只有一个非零元了。按第 $n$ 列展开，我们得到 $det(P-\lambda I) = (1-\lambda) det(Q-\lambda I’)$。其中$Q$ 为 $n-1$ 阶方阵，且$Q_{ij} = a_{ij} - a_{nj}$；$I’$ 是 $n-1$ 阶单位矩阵。这是很容易验证的。</p><p>这时候 $Q$ 必有特征值 $1$. </p><p>接下来我们考虑 $Q$ 关于 $1$ 的<strong>行特征向量</strong>（注意是<strong>行特征向量</strong>，即 $Q^T$ 的列特征向量）$\beta = (\beta_1,\beta_2,..\beta_{n-1})$。</p><p>我们有 $$(\beta Q)_j = \displaystyle{\sum_{i=1}^{n-1}\beta_i(a_{ij}-a_{nj})=\beta_j}$$ </p><p>接下来讨论 $\beta$ 中元素的正负性。设有 $p$ 个正元素（分别为 $\beta_{s_1},\beta_{s_2},..\beta_{s_p}$）; $q$ 个非正元素（分别为$\beta_{t_1},\beta_{t_2},..\beta_{t_q}$）。另外我们不妨设$\rho = |\beta|\geqslant 0$；因为如果小于 $0$， 我们可以取 $-\beta$ 作为我们讨论的对象。</p><p>这样，对 $\beta$ 中的正元素，我们可以将上面的式子改写为：</p>   $$  \displaystyle{\sum_{i=1}^{n-1}\beta_ia_{it_k}-\rho a_{nt_k}=\beta_{t_k}}\space\space (k = 1,2,...p)$$    累加，得到：   $$  \displaystyle{\sum_{i=1}^{n-1}\beta_i(\sum_{k = 1}^{p}a_{it_k})-\rho( \sum_{k=1}^{p}a_{nt_k})=\sum_{k=1}^p\beta_{t_k}}$$    <p>为了简化式子，不妨令 $d_i = \displaystyle{\sum_{k=1}^{p}a_{it_k} < \sum_{i=1}^{n}a_i = 1}$ ：</p>   $$  \displaystyle{\sum_{i=1}^{n-1}\beta_id_i-\rho d_n=\sum_{k=1}^p\beta_{t_k}}$$    $$  \displaystyle{\sum_{k=1}^{p}\beta_{s_k}d_{s_k} + \sum_{k=1}^{q}\beta_{t_k}d_{t_k}-\rho d_n=\sum_{k=1}^p\beta_{t_k}}$$   $$  \displaystyle{\sum_{k=1}^{q}\beta_{t_k}d_{t_k}-\rho d_n=\sum_{k=1}^p\beta_{t_k} - \sum_{k=1}^{p}\beta_{s_k}d_{s_k} > 0}$$   <p>而等式左边显然是负数，矛盾。这说明 $Q$ 没有特征值 $1$， 也就是说 $P$ 的特征值 $1$ 无重根。</p></li></ol><p>回到我们对极限的讨论。如果这个极限 $L$ 存在，那么它会满足：$$LP = L$$</p><p>也就是说它的每一行都是 $P$ 关于特征值 $1$ 的行特征向量； 我们刚刚证明$1$ 这个特征值无重根，那么这个行特征向量是唯一的。</p><p>—— 是的， $L$ 的每一行都相同，均为 $P$ 关于 $1$ 的行特征向量。</p><h4 id="极限何时存在？"><a href="#极限何时存在？" class="headerlink" title="极限何时存在？"></a>极限何时存在？</h4><p>【 最近事务繁多，更新无限期推迟】</p><p>有任何错误，请在评论区指正或者给我发邮件。（评论使用Disqus系统，可能需要翻墙）</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;马尔可夫链（Markov chain）&lt;/strong&gt;是数学建模和机器学习常用的工具（据说尤其在&lt;strong&gt;NLP&lt;/strong&gt;中，我目前尚不了解很多，但之前曾看过一篇&lt;a href=&quot;https://codeburst.io/how-i-gene</summary>
      
    
    
    
    <category term="math" scheme="https://huangweiran.club/categories/math/"/>
    
    
    <category term="programming" scheme="https://huangweiran.club/tags/programming/"/>
    
    <category term="learning" scheme="https://huangweiran.club/tags/learning/"/>
    
  </entry>
  
  <entry>
    <title>南朝金粉太平春，万里山河处处青</title>
    <link href="https://huangweiran.club/2018/09/02/%E5%8D%97%E6%9C%9D%E9%87%91%E7%B2%89%E5%A4%AA%E5%B9%B3%E6%98%A5%EF%BC%8C%E4%B8%87%E9%87%8C%E5%B1%B1%E6%B2%B3%E5%A4%84%E5%A4%84%E9%9D%92/"/>
    <id>https://huangweiran.club/2018/09/02/%E5%8D%97%E6%9C%9D%E9%87%91%E7%B2%89%E5%A4%AA%E5%B9%B3%E6%98%A5%EF%BC%8C%E4%B8%87%E9%87%8C%E5%B1%B1%E6%B2%B3%E5%A4%84%E5%A4%84%E9%9D%92/</id>
    <published>2018-09-03T04:34:54.000Z</published>
    <updated>2021-06-29T08:24:42.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>西郊芳草年年绿，多少游人似去年？</strong></p><p>《金粉世家》的小说正篇以这样一句诗作结。</p><p>这北京西郊的芳草，当年曾亲见金总理家的七少爷在信马游春时，对一个穿着青色衣裙的女学生一见倾心；如今又见证了八小姐梅丽和谢玉树轮蹄相驰，互萌爱意。这本是何其浪漫的两个场景，却因为前一对浪漫主角的凄然结局而显得有些讽刺。</p><p>每一位读完《金粉世家》的读者想必都会思考这样一个问题：<strong>金燕西到底有没有爱过冷清秋？</strong></p><p>在我看来，答案很显然是：<strong>没有</strong>。</p><p>或者更严谨一点，从”爱“这个词的一般定义来看，是没有的；而以金燕西自己的标准来看，是有的，并且根据这样的标准，他爱过的人还包括白秀珠，白莲花，白玉花，小怜，等等，但他的这种爱需要新鲜感来维系。</p><p>冷清秋一直以为，他们爱情的最终失败源于四个字：<strong>齐大非偶</strong>。但门第的差距并不是真正的原因；作者已经悄悄给我们提供了几个反例：无论是柳春江和小怜，还是卫璧安和吴蔼芳，双方的门第差距都不比金冷二者小，甚至更大，但他们都获得了幸福的结局。读者也可以看到，除了三少奶奶王玉芬因为白家对清秋怀恨在心之外，她并没有受到多少来自家庭的阻隔。金家几位姐妹以及金太太都非常喜欢她，而她的为人亲和也赢得了仆人们的尊重。</p><p>但爱情终究是两个人的事情，她在婚姻中遇到的一切不如意其实都是因为她曾经的佳偶金燕西。所以，如果一定要用四个字描述他们悲剧的原因，我想应该是：<strong>遇人不淑</strong>。</p><p>只看了开头的读者也许会误把金燕西当作一个痴情公子，这也是作者刻意要营造的假象。与此同时，他也在细微处让金燕西流露出自己的本性。</p><p>读者会记得，金燕西西直门外对冷清秋一见钟情，随后花了一些力气找到了她家，从而展开了自己的追求攻势。但他们也许会忘了下面这段对话：</p><blockquote><p>金荣笑道：“七爷，你要找的那个人，给你找到了。” </p><p>燕西道：“我要找谁？” </p><p>金荣笑道：“七爷很挂心的一个人。” </p><p>燕西道：“我挂心的是谁？我越发不明白你这话了。” </p><p>金荣道：“七爷就全忘了吗？那天在海淀看到的那个人呢。”</p></blockquote><p>——这里可以看出，他对冷清秋这个人其实并没有那么挂念，如果不是金荣提起，恐怕已经全忘了。</p><p>但就在当天，在还不知道冷清秋的名字的情况下，他第一次萌生了要娶她的念头：</p><blockquote><p>一回房，便想起落花胡同那个女孩子，心想，老大的话，果然不错。若说交女朋友，自然是交际场中新式的女子好。<strong>但是要结为百年的伴侣，主持家事，又是朴实些的好。若是我把那个女孩子娶了回来，我想她的爱情，一定是纯一的，人也是很温和的，决不象交际场中的女子，不但不能干涉她的行动，她还要干涉你的行动啦</strong>。就以姿色论，那种的自然美，比交际场中脂粉堆里跳出来的人，还要好些呢。好，就是这样办。</p></blockquote><p>——读者于是知道，他需要的并不是冷清秋，而是一个旧式的女子：爱情纯一，能主持家事，又不干涉他在外结交女朋友。事实证明他没有看错人，婚后的冷清秋的确是这么做的。</p><p>之后二人的关系，在金燕西的攻势下逐渐升温，并在第一次西山之行达到一个小顶点。</p><p>在西山，金燕西对冷清秋做出了真挚的表白：</p><blockquote><p>燕西道：“我不是说了吗？婚姻自由，他们是不能过问的。只要你不嫌弃我，这事就成立了。慢说他们不能不赞成，就是实行反对，他还能打破我们这婚约吗？你若是拒绝我的要求，就请你明说。不然，为了两家门第的关系，将我们纯洁的爱情发生障碍，那未免因小失大。<strong>而且爱情的结合，只要纯正，就是有压力来干涉，也要冒万死去反抗，</strong>何况现在并没有什么阻碍发生呢？”</p></blockquote><p>金燕西说这些话的时候，一定是相信自己深爱着冷清秋的。他觉得两人的爱情就是真正的新式爱情，没有来自长辈的干涉，没有门当户对的成见。因此冷清秋也信了。尽管她一直有着齐大非偶的担忧，却最终仍然接受了他送的戒指。</p><p>但清秋对于燕西究竟是个怎样的人也非毫无知觉。她深知“月满则缺，水满则倾”，因此表达了希望二人“生疏些”的愿望；在热恋之际，两人赏月的美好时刻，她说出“多情自古空余恨，好梦由来最易醒”这样悲怆的话语，也能看出她对未来的担忧；在第二次西山之行，她甚至和燕西有了这样的对话：</p><blockquote><p>清秋笑道：“你所认识的女朋友，有小姐、有女学生、有戏子，还有交际明星，岂不是交际很广？”</p><p>燕西道：“这是哪里来的谣言？全没有这回事。”</p><p>清秋笑道：“管他有没有，大家心里明白就是了。”</p></blockquote><p>尽管如此，这天晚上，她还是把自己的处女之身献给了金燕西，而这也成了他们悲剧婚姻的起点。</p><p>《金粉世家》这本书，在我看来，有两个转折点。第一个转折点，是金冷二人的婚礼：自此之后，小说中再也没有二人的甜蜜，爱情逐渐滑入悲剧；第二个转折点，就是金铨之死，金家由此分崩离析，树倒猢狲散。</p><p>婚后燕西对清秋态度的转变，这里不详细讲述。我们只消看下面这几个例子。</p><p>在帮助刘宝善出狱后，燕西回家。在听了清秋一番分析后，</p><blockquote><p>燕西觉得夫人如此聪明，说得又如此可怜，不觉心动，望着夫人的脸，只管注意。<strong>男女之间，真是有一种神秘，这一下子，燕西夫妇又回复到了新婚时代了</strong>。</p></blockquote><p>在此之前，二人已不似从前那般甜蜜。这次只因燕西在清秋身上又找到了一些神秘感，才使二人感情有所回温。</p><p>之后，当二人感情彻底破裂，清秋的独立意识觉醒，决心离婚时，燕西和梅丽有了这样一番对话：</p><blockquote><p>梅丽冷笑道：“你说这话，以为夫妻拆开，也像主人辞退一个下人一样呢。” </p><p>燕西道：“那本来没有什么分别。”</p></blockquote><p>而当清秋最终携子“火遁”之后，他则有这样的心理活动：</p><blockquote><p>我又不是不让她离婚，何必有这种行动？</p><p>是了，一定是怕我要回小孩来，所以带着他隐藏起来了。</p><p>其实我不过二十岁的人，哪里会愁到没有孩子？你带了去就只管带了去，我是丝毫也不关痛痒的。</p></blockquote><p>至此，金燕西的真面目彻底地展示在了读者面前。从头到尾，他对冷清秋的感情，不过是一个纨绔子弟对怀春少女的新鲜感，新鲜感一消退，这所谓感情也就不存在了。</p><p>冷清秋最终也是明白这个道理的。她承认，自己是<strong>“受着物质与虚荣的引诱，就把持不定地嫁给了燕西”，“结果是卖了自己的身子，来受人家奚落”</strong>。</p><p>我们这时候把角度放大，其实金家其他几个子弟又何尝不是跟金燕西一样呢？</p><p>老大凤举在外养妾，让人骗走身家；其他几位包括燕西都在外追捧戏子。他们整日花天酒地，不学无术，做家庭里的寄生虫。在“宿主”金铨意外身亡后，这些寄生虫也就现出原形，无处求生了。金燕西本人，则是从一个富贵公子，沦为了一个出演电影的戏子。</p><p>此时我们再看回小说的标题。</p><p><strong>“金粉世家”之“金粉”，何也？我以为，是南朝之金粉也。</strong>彼时宋齐梁陈几朝，偏居江南，国力日衰，国君生活却绮艳奢靡。四朝最后都短命而终。昔人有诗句：“南朝四百八十寺，多少楼台烟雨中。”</p><p>我无意对小说主旨深追，读者看了心中自有定论。</p><p>写到这里，本文也该结束了。作为一篇书评，这篇文章的展开是以金燕西为线索的。由于我的懒惰，这条线索也没能写得详尽（因为写太长的确很累，写到中间实际上已经懒于去书中寻找线索了）；但冷清秋一线，实在也非常重要。读了小说的朋友会发现，作者在文中不断提到新旧思想的碰撞交融。金家是名义上的新式家庭，在很多地方却流于旧式；冷清秋大体上是个旧式女子，却在最后有了新式女子独立意识的觉醒。展开来写的话，我想这会是一个非常有趣的主题。但我实在不是一个中文系的学生，对文学批评只有很粗浅的一些想法。因此这里，我谨摘录冷清秋与金燕西的诀别书与诸位分享，以此为此文作结。这篇诀别书非常精彩，将冷清秋独立意识的觉醒展现得淋漓尽致，于全书实在是画龙点睛之妙。</p><p>最后顺嘴说一句，这本书开年读完实在有些遗憾，因为我不知道这一整年我能否再读到一本能与之相比的小说了。</p><blockquote><p>燕西先生文鉴：</p><p>​        西楼一火，劳燕遂分，别来想无恙也。秋此次不辞而别，他人必均骇然，而先生又必独欣然。<strong>秋对于欣然者，固无所用其不怿</strong>，而对于骇然者，亦终感未能木然置之。何也？知者谓我逃世，不知者谓我将琵琶别抱也。再四思维，于是不得不有此信之告矣。<br>​       秋出走之初，原拟携此呱呱之物，直赴西郊，于昆明湖畔，觅一死所。继思此呱呱之物，果何所知？而亦遭此池鱼之殃。况吾家五旬老母，亦唯秋一点骨肉，秋果自尽，彼孑然 一身，又何生为？秋一死不足惜，而更连累此一老一少。天地有好生之德，窃所不忍也。为此一念徘徊郊外，久不能决。凡人之求死，只在最初之五分钟，此五分钟犹豫既过，勇气顿失，愈不能死。于是秋遂薄暮返城，托迹女友之家，一面函告家母，约予会见。<br>​       家母初以秋出走非是，冀覆水之重收。<strong>此秋再三陈以利害，谓合则在君势如仇敌，在秋形同牢囚。人生行乐耳，乃为旧道德之故，保持夫妻名义，行尸走肉，断送一生，有何趣味？若令秋入金门，则是宣告我无期徒刑，入死囚之牢也。</strong><br>​       家母见秋之志已决，无可挽回，于是亦毅然从秋之志，愿秋与君离异，以 另谋新生命。惟是秋转念择人不慎，中道而去，知者以为君实不德，秋扇见捐，不知者以为秋高自攀附，致遭白眼。则读书十年，所学何事？夫赵孟所贵，赶孟能贱之，本不足怪。然齐大非偶，古有明训，秋幼习是言，而长乃昧于是义，是秋之有今日，秋自取之。<br>​       而今而后，尚何颜以冷清秋三字，以与社会相见乎？因是秋遂与母约，扬言秋已步三闾大夫后少，葬身于昆明湖内，从此即隐姓埋名，举家而遁于他方。金冷婚约，不解而解矣。<br>​       秋家今已何往？君可不问。至携一子，为金门之骨肉，本不应与同往。然而君且无伉俪 之情，更何有父子之义？置儿君侧，君纵听之，而君所获之新爱人，宁能不视此为眼中钉，拔去之而后快耶？与其将来受人非种必锄之举，则不如秋保护之，延其一线之生命也。俟其长大，自当告以弃儿之身世，一日君或欲一睹此赘疣，当尚有机缘也。<br>​        行矣！ 燕西。<strong>生生世世，吾侪不必再晤</strong>。此信请为保留，即作为绝交之书，离婚之约。万一君之新夫人以前妻葛藤未断为嫌，则以此信视之可也。<br>​        行矣！燕西。君子绝交，不出恶声，秋虽非君子，既对君钟情于前，亦雅不欲于今日作无味之争论。然而临别赠言，有未能已者，语云：<strong>高明之家，鬼瞰其室，虎尾春冰，宜有以防其渐</strong>。以先翁位高德茂，继祖业而起来兹，本无可议。<strong>若至晚辈，则南朝金粉之香，冠盖京华之盛，未免兼取而并进，是非青年所以自处之道也</strong>。愿有则改之，无则加勉焉。<br>​        慈姑老大人，一年以来，抚秋如己出，实深感戴。寸恩未报，会当衔结于来生。此外妯娌姊妹，对秋亦多加爱护，而四姊八妹，一则古道热肠，肝胆相照，一则耳鬓厮磨，形影相惜。今虽飘泊风尘，而夜雨青灯，每一回忆，宁不感怀？故秋虽去，而寸心耿耿，犹不免神驰左右。顾人生百年，无不散之筵席，均毋以秋为念可也。蓬窗茅户，几榻生尘。伏案作书，恍如隔世。言为心声，泪随笔下。楮尽墨枯，难述所怀。专此奉达，并祝健康！ </p><p>​                                                                                      冷清秋谨启 </p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;西郊芳草年年绿，多少游人似去年？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;《金粉世家》的小说正篇以这样一句诗作结。&lt;/p&gt;
&lt;p&gt;这北京西郊的芳草，当年曾亲见金总理家的七少爷在信马游春时，对一个穿着青色衣裙的女学生一见倾心；如今又见证了八小姐梅丽和谢玉树轮蹄相驰，互</summary>
      
    
    
    
    <category term="Literature" scheme="https://huangweiran.club/categories/Literature/"/>
    
    
    <category term="book review" scheme="https://huangweiran.club/tags/book-review/"/>
    
    <category term="novels" scheme="https://huangweiran.club/tags/novels/"/>
    
  </entry>
  
  <entry>
    <title>CS 224N: Assignment #1</title>
    <link href="https://huangweiran.club/2018/08/30/CS-224N-Assignment-1/"/>
    <id>https://huangweiran.club/2018/08/30/CS-224N-Assignment-1/</id>
    <published>2018-08-30T19:15:27.000Z</published>
    <updated>2019-09-02T10:13:40.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Word2vec-Summary"><a href="#Word2vec-Summary" class="headerlink" title="Word2vec Summary"></a>Word2vec Summary</h2><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><ol><li><p>One-hot没有在表达中体现词的相似度，并且空间浪费。</p></li><li><p>SVD-based的方法是先构造cooccurrence矩阵（这个共同出现可能会被限制在某个window范围内），然后对矩阵使用SVD分解，取前k个奇异值得到矩阵 $U$ 就得到每个词的k维表达。但这种做法弊端明显：</p><ul><li><p>新词的出现和语料的改变会造成矩阵尺寸频繁变化。</p></li><li><p>矩阵非常稀疏，并且维度通常很大。</p></li><li><p>SVD分解有平方级的开销，而我们刚刚说矩阵的维度很大，因此这个开销会很恼人。</p></li><li><p>需要对不同词语出现频率悬殊做额外处理。</p></li></ul><p>有一些针对的解决办法，但不够好。</p></li></ol><h3 id="Word2vec：iteration-based"><a href="#Word2vec：iteration-based" class="headerlink" title="Word2vec：iteration based"></a>Word2vec：iteration based</h3><p>word2vec的参数就是词向量矩阵本身，它通过迭代的方式训练。它包括两种算法：连续词袋模型（CBOW）和skip-gram。前者通过上下文预测中心词，后者反之。训练方法也有两种：负采样和层次softmax。</p><h3 id="CBOW"><a href="#CBOW" class="headerlink" title="CBOW"></a>CBOW</h3><p>$U$ 为输出矩阵，在这里即中心词的embedding矩阵；$V$ 为输入矩阵，在这里是上下文词语的embedding矩阵。</p><p>算法流程如下：</p><ol><li>取窗口内（窗口大小为m）的词语的one-hot vector。</li><li>对其在矩阵 $V$ 中做embedding lookup操作（其实就是每个vector乘上这个矩阵），得到2m个词向量。</li><li>直接对这2m个向量取平均。（这也是为什么它被称作连续词袋模型，它仍然不考虑这些词语的顺序，像把词向量的每个维度当成了袋子一样收集每个词提供的信息。）</li><li>把这个向量与 $U$ 相乘，实际上就是和每个中心词做了内积，然后对得到的向量做softmax，就是我们得到的概率值。</li></ol><p>loss function为这个概率和实际结果（其实是一个one-hot vector，可以认为它在正确结果处概率为1）的交叉熵。交叉熵是信息论中的概念，可以用来刻画两个分布的不同。由于真实分布只在一个点处有取值，且是1，<strong>其实这个交叉熵就是输出概率值在正确类对应的index上的熵</strong>。</p><p>故我们的优化目标是：</p>$$\begin{aligned} \text { minimize } J &=-\log P\left(w_{c} | w_{c-m}, \ldots, w_{c-1}, w_{c+1}, \ldots, w_{c+m}\right) \\ &=-\log P\left(u_{c} | \hat{v}\right) \\ &=-\log \frac{\exp \left(u_{c}^{T} \hat{v}\right)}{\sum_{j=1}^{|V|} \exp \left(u_{j}^{T} \hat{v}\right)} \\ &=-u_{c}^{T} \hat{v}+\log \sum_{j=1}^{|V|} \exp \left(u_{j}^{T} \hat{v}\right) \end{aligned}$$<p>接下来的工作交给SGD。</p><h3 id="Skip-gram"><a href="#Skip-gram" class="headerlink" title="Skip-gram"></a>Skip-gram</h3><p>$U,V$ 仍然分别代表输出和输入矩阵，只是他们代表的词语位置和刚才相反。</p><p>算法流程和上面基本类似，这里简单描述：</p><ol><li>用中心词的one-hot vector获取其embedding。</li><li>与output matrix相乘，再做softmax。</li><li>用得到的概率向量和真实输出（就是一个只有窗口范围内是1，其余都是0的向量）做交叉熵，作为loss。</li></ol>$$\begin{aligned} \text { minimize } J &=-\log P\left(w_{c-m}, \ldots, w_{c-1}, w_{c+1}, \ldots, w_{c+m} | w_{c}\right) \\ &=-\log \prod_{j=0, j \neq m}^{2 m} P\left(w_{c-m+j} | w_{c}\right) \\ &=-\log \prod_{j=0, j \neq m}^{2 m} P\left(u_{c-m+j} | v_{c}\right) \\ &=-\log \prod_{j=0, j \neq m}^{2 m} \frac{\exp \left(u_{c-m+j}^{T} v_{c}\right)}{\sum_{k=1}^{|V|} \exp \left(u_{k}^{T} v_{c}\right)} \\ &=-\sum_{j=0, j \neq m}^{2 m} u_{c-m+j}^{T} v_{c}+2 m \log \sum_{k=1}^{|V|} \exp \left(u_{k}^{T} v_{c}\right) \end{aligned}$$<h3 id="Negative-Sampling"><a href="#Negative-Sampling" class="headerlink" title="Negative Sampling"></a>Negative Sampling</h3><p>注意到上面的算法中softmax开销非常大。为了缩小这个开销我们可以考虑不直接求，而是取近似。</p><p>直觉上，我们softmax的目标是让窗口内的上下文词语输出大，而让其他词语输出小。把窗口内的词语当作正样本，其他词语当作负样本，如果我们进行负采样的话，在目标函数中同时令前者产生的概率大，后者产生的概率小，就近似达到目标效果了。</p><p>这里我们把刻画一个样本的概率函数换为sigmoid，采用最大似然的方法设计loss function，能得到如下式子：</p>$$J=-\sum_{(w, c) \in D} \log \frac{1}{1+\exp \left(-u_{w}^{T} v_{c}\right)}-\sum_{(w, c) \in D} \log \left(\frac{1}{1+\exp \left(u_{w}^{T} v_{c}\right)}\right)$$<p>具体地，对于skip-gram，在上下文词语 $c-m+j$ 上的目标函数是：</p>$$-\log \sigma\left(u_{c-m+j}^{T} \cdot v_{c}\right)-\sum_{k=1}^{K} \log \sigma\left(-\tilde{u}_{k}^{T} \cdot v_{c}\right)$$<p>CBOW类似。</p><p>K个负样本是从某种分布中抽取的。经验上来说这个分布使用$\dfrac{3}{4}$次方的unigram模型是最好的。选取$\dfrac{3}{4}$的原因简单来说就是对于接近1的数，它的值基本维持，但对于很小的数，它的值会变大，能使它更容易被采样到。对于像”is”（0.9）这样的词语，在这样的情况下变化就很小，而像“bombastic”（0.01）则能获得3倍于之前的采样概率。</p><h3 id="Hierarchical-Softmax"><a href="#Hierarchical-Softmax" class="headerlink" title="Hierarchical Softmax"></a>Hierarchical Softmax</h3><p>通过构造一棵二叉树的办法来使得每次获得某一个词的概率时不用遍历整个词典。这棵树上叶子节点表示一个词，除了根节点和叶子节点，每一个中间节点都有一个自己的embedding。在给定一个vector的情况下，要求概率需要遍历从根节点到该词所对应的叶子节点的路径，累乘到达每个中间节点的概率。这个概率用sigmoid表示，且每条路径上下一步如果选择左子节点，概率为P(X=1)的情况，右子节点则为P(X=0)的情况，这样实际上也是一种normalization，保证概率之和为1。</p><p>这个方法的速度取决于树是如何构造的。在Mikolov的论文中他们使用的树是哈夫曼树，这种树的特点是会给出现频率高的叶子节点更短的路径。</p><p>接下来是作业的部分。经过对比，我认为2019以前的第一次作业要比2019的这个作业有趣一些，因此我选择了旧版本的第一个编程作业来完成。</p><h2 id="1-Softmax"><a href="#1-Softmax" class="headerlink" title="1. Softmax"></a>1. Softmax</h2><blockquote><p> (a) Prove that softmax is invariant to constant offsets in the input, that is, for any input vector $\bf{x}$ and any constant $c$<br>$$<br>\mathbf{softmax}(\mathbf{x}) = \mathbf{softmax}(\mathbf{x} + c)<br>$$<br>where $ \bf{x} + c$ means adding the constant $c$ to every dimension of $\bf{x}$. Remember that  $$\mathbf{softmax}(\mathbf{x})_i = \dfrac{e^{x_i}}{\sum_j e^{x_j}}$$</p></blockquote><p>这个证明是显然的，我们只需要在softmax的分子分母中同时乘上 $e^c$这一项即可。但这个性质在我们实现softmax的时候非常重要。</p><p>考虑这样的情形，你需要对 $\mathbf{x} = [1001, 1002]$ 做softmax运算，但 $e^{1001}$ 毫无疑问是溢出的。但对于softmax而言，$[1001, 1002]$ 和 $[1,2]$ 又有什么区别呢？因此实际实现的时候，我们通常取 $c = - \mathbf{max}\space x_i$  以保证数值上的稳定。</p><blockquote><p>(b) Given an input matrix of N rows and D columns, compute the softmax prediction for each row using the optimization in part (a). Write your implementation in <code>q1_softmax.py</code>. You may test by executing python <code>q1_softmax.py</code>.</p></blockquote><p>代码如下：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">softmax</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""Compute the softmax function for each row of the input x.    You should also make sure that your code works for a single    D-dimensional vector (treat the vector as a single row) and    for N x D matrices. This may be useful for testing later. Also,    make sure that the dimensions of the output match the input.    Arguments:    x -- A D dimensional vector or N x D dimensional numpy matrix.    Return:    x -- You are allowed to modify x in-place    """</span>    orig_shape <span class="token operator">=</span> x<span class="token punctuation">.</span>shape    <span class="token keyword">if</span> len<span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token operator">></span> <span class="token number">1</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># Matrix</span>        <span class="token comment" spellcheck="true"># scale down</span>        x_max <span class="token operator">=</span> np<span class="token punctuation">.</span>max<span class="token punctuation">(</span>x<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>        x <span class="token operator">-=</span> x_max        <span class="token comment" spellcheck="true"># softmax process</span>        x <span class="token operator">=</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        exp_sum <span class="token operator">=</span> np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>x<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>        x <span class="token operator">/=</span> exp_sum    <span class="token keyword">else</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># Vector</span>        <span class="token comment" spellcheck="true"># scale down</span>        x_max <span class="token operator">=</span> np<span class="token punctuation">.</span>max<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">-=</span> x_max        <span class="token comment" spellcheck="true"># softmax process</span>        x <span class="token operator">=</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">/=</span> np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>x<span class="token punctuation">)</span>    <span class="token keyword">assert</span> x<span class="token punctuation">.</span>shape <span class="token operator">==</span> orig_shape    <span class="token keyword">return</span> x</code></pre><h2 id="2-Neural-Network-Basics"><a href="#2-Neural-Network-Basics" class="headerlink" title="2. Neural Network Basics"></a>2. Neural Network Basics</h2><blockquote><p>(a) Derive the gradients of the sigmoid function and show that it can be rewritten as a function of the function value (i.e., in some expression where only $\sigma(x)$, but not $x$, is present). Assume that the input $x$ is a scalar for this question. Recall, the sigmoid function is $\sigma(x) = \dfrac{1} {1 + e^{−x}}$ .</p></blockquote><p>这个推导非常简单：$(\sigma(x))’ = \sigma(x)(1 - \sigma(x))$。</p><blockquote><p>(b) ) Derive the gradient with regard to the inputs of a softmax function when <strong>cross entropy loss</strong> is used for evaluation, i.e., find the gradients with respect to the softmax input vector $\theta$, when the prediction is made by $\hat{y} = \mathbf{softmax}(\theta)$. Remember the cross entropy function is<br>$$<br>\mathbf{CE}(\mathbf{y}, \mathbf{\hat{y}}) = -\sum_i y_i\log \hat{y}_i<br>$$<br> where $\mathbf{y}$ is the one-hot label vector, and $\mathbf{\hat{y}}$ is the predicted probability vector for all classes.</p></blockquote><p>我们知道 $\mathbf{y}$ 是一个one-hot label vector，这就是说它只有一项是 $1$ , 其余项均为  $0$ 。设 $\mathbf{y}$ 的第 $i$ 项为 $1$.</p><p>对第 $i$ 项： $\dfrac{\partial{\mathbf{CE}}}{\partial{\theta_i}} = \dfrac{\partial{\mathbf{CE}}}{\partial{\hat{y_i}}} \dfrac{\partial{\hat{y_i}}}{\partial{\theta_i}} = -\dfrac{1}{\hat{y_i}}\dfrac{\partial \space\mathbf{softmax}(\theta)_i} {\partial \theta_i}$</p><p> 而 $\dfrac{\partial \space\mathbf{softmax}(\theta)_i} {\partial \theta_i}  =  \dfrac{e^{\theta_i}(\sum_j e^{\theta_j}) - e^{\theta_i}e^{\theta_i}}{(\sum_j e^{\theta_j})^2}= \mathbf{softmax}(\theta)_i(1 - \mathbf{softmax}(\theta)_i) = \hat{y_i}(1-\hat{y_i})$</p><p>故 $\dfrac{\partial{\mathbf{CE}}}{\partial{\theta}_i} = \hat{y_i} - 1$</p><p>对第 $j \not = i$ 项： $\dfrac{\partial{\mathbf{CE}}}{\partial{\theta_j}} = \dfrac{\partial{\mathbf{CE}}}{\partial{\hat{y_i}}} \dfrac{\partial{\hat{y_i}}}{\partial{\theta_j}} = -\dfrac{1}{\hat{y_i}}\dfrac{\partial \space\mathbf{softmax}(\theta)_i} {\partial \theta_j}$</p><p>而 $\dfrac{\partial \space\mathbf{softmax}(\theta)_i} {\partial \theta_j} = \dfrac{0(\sum_j e^{x_j}) - e^{x_j}e^{x_i}}{(\sum_j e^{x_j})^2} = -\hat{y_i}\hat{y_j}$</p><p>故 $\dfrac{\partial{\mathbf{CE}}}{\partial{\theta}_j} = \hat{y_j} = \hat{y_j} - 0$</p><p>可以看出两种情况可以综合为 $\hat{y_i} - y_i$ 的形式，则<br>$$<br>\dfrac{\partial{\mathbf{CE}}}{\partial{\theta}} = \hat{y} - y<br>$$</p><blockquote><p>(c) Derive the gradients with respect to the inputs x to an one-hidden-layer neural network (that is, find $\dfrac{\partial{J}}{\partial{x}}$ where $J = \mathbf{CE}(y, \hat{y})$ is the cost function for the neural network). The neural network employs sigmoid activation function for the hidden layer, and softmax for the output layer. Assume the one-hot label vector is y, and cross entropy cost is used. (Feel free to use $\sigma ‘ (x)$ as the shorthand for sigmoid gradient, and feel free to define any variables whenever you see fit.) </p></blockquote>根据题意，我们有这样一个两层神经网络：$$\boldsymbol{h}=\operatorname{sigmoid}\left(\boldsymbol{x} \boldsymbol{W}_{1}+\boldsymbol{b}_{1}\right) \quad \hat{\boldsymbol{y}}=\operatorname{softmax}\left(\boldsymbol{h} \boldsymbol{W}_{2}+\boldsymbol{b}_{2}\right)$$记 $z_{1} = \boldsymbol{xW}_{1} + \boldsymbol{b}_{1}$ ， $z_{2} = \boldsymbol{hW}_{2} + \boldsymbol{b}_{2}$；$$\begin{aligned} \boldsymbol{\delta}_{1} &=\frac{\partial C E}{\partial z_{2}}=\hat{\boldsymbol{y}}-\boldsymbol{y} \\ \boldsymbol{\delta}_{2} &=\frac{\partial C E}{\partial \boldsymbol{h}}=\boldsymbol{\delta}_{1} \frac{\partial \boldsymbol{z}_{2}}{\partial \boldsymbol{h}}=\boldsymbol{\delta}_{1} \boldsymbol{W}_{2}^{\top} \\ \boldsymbol{\delta}_{3} &=\frac{\partial C E}{\boldsymbol{z}_{1}}=\boldsymbol{\delta}_{2} \frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}_{1}}=\boldsymbol{\delta}_{2} \circ \sigma^{\prime}\left(\boldsymbol{z}_{1}\right) \\ \frac{\partial C E}{\partial \boldsymbol{x}} &=\boldsymbol{\delta}_{3} \frac{\partial \boldsymbol{z}_{1}}{\partial \boldsymbol{x}}=\boldsymbol{\delta}_{3} \boldsymbol{W}_{1}^{\top} \end{aligned}$$<p>这个神经网络有 $ (D_x + 1) \cdot H + (H + 1) \cdot D_y$个参数。</p><p>接下来是完成这个网络的代码。先完成<code>gradient_check</code>的部分：</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># First implement a gradient checker by filling in the following functions</span><span class="token keyword">def</span> <span class="token function">gradcheck_naive</span><span class="token punctuation">(</span>f<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">""" Gradient check for a function f.    Arguments:    f -- a function that takes a single argument and outputs the         cost and its gradients    x -- the point (numpy array) to check the gradient at    """</span>    rndstate <span class="token operator">=</span> random<span class="token punctuation">.</span>getstate<span class="token punctuation">(</span><span class="token punctuation">)</span>    random<span class="token punctuation">.</span>setstate<span class="token punctuation">(</span>rndstate<span class="token punctuation">)</span>    fx<span class="token punctuation">,</span> grad <span class="token operator">=</span> f<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># Evaluate function value at original point</span>    h <span class="token operator">=</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">4</span>        <span class="token comment" spellcheck="true"># Do not change this!</span>    <span class="token comment" spellcheck="true"># Iterate over all indexes ix in x to check the gradient.</span>    it <span class="token operator">=</span> np<span class="token punctuation">.</span>nditer<span class="token punctuation">(</span>x<span class="token punctuation">,</span> flags<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'multi_index'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> op_flags<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'readwrite'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">while</span> <span class="token operator">not</span> it<span class="token punctuation">.</span>finished<span class="token punctuation">:</span>        ix <span class="token operator">=</span> it<span class="token punctuation">.</span>multi_index        x<span class="token punctuation">[</span>ix<span class="token punctuation">]</span> <span class="token operator">+=</span> h        fx_forward<span class="token punctuation">,</span> _ <span class="token operator">=</span> f<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x<span class="token punctuation">[</span>ix<span class="token punctuation">]</span> <span class="token operator">-=</span> <span class="token number">2</span><span class="token operator">*</span>h        fx_backward<span class="token punctuation">,</span> _ <span class="token operator">=</span> f<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x<span class="token punctuation">[</span>ix<span class="token punctuation">]</span> <span class="token operator">+=</span> h        numgrad <span class="token operator">=</span> <span class="token punctuation">(</span>fx_forward <span class="token operator">-</span> fx_backward<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">*</span> h<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Compare gradients</span>        reldiff <span class="token operator">=</span> abs<span class="token punctuation">(</span>numgrad <span class="token operator">-</span> grad<span class="token punctuation">[</span>ix<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">/</span> max<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> abs<span class="token punctuation">(</span>numgrad<span class="token punctuation">)</span><span class="token punctuation">,</span> abs<span class="token punctuation">(</span>grad<span class="token punctuation">[</span>ix<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> reldiff <span class="token operator">></span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">5</span><span class="token punctuation">:</span>            <span class="token keyword">print</span> <span class="token string">"Gradient check failed."</span>            <span class="token keyword">print</span> <span class="token string">"First gradient error found at index %s"</span> <span class="token operator">%</span> str<span class="token punctuation">(</span>ix<span class="token punctuation">)</span>            <span class="token keyword">print</span> <span class="token string">"Your gradient: %f \t Numerical gradient: %f"</span> <span class="token operator">%</span> <span class="token punctuation">(</span>                grad<span class="token punctuation">[</span>ix<span class="token punctuation">]</span><span class="token punctuation">,</span> numgrad<span class="token punctuation">)</span>            <span class="token keyword">return</span>        it<span class="token punctuation">.</span>iternext<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># Step to next dimension</span>    <span class="token keyword">print</span> <span class="token string">"Gradient check passed!"</span></code></pre><p>然后完成这个二层MLP的的函数：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">forward_backward_prop</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> labels<span class="token punctuation">,</span> params<span class="token punctuation">,</span> dimensions<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    Forward and backward propagation for a two-layer sigmoidal network    Compute the forward propagation and for the cross entropy cost,    the backward propagation for the gradients for all parameters.    Notice the gradients computed here are different from the gradients in    the assignment sheet: they are w.r.t. weights, not inputs.    Arguments:    X -- M x Dx matrix, where each row is a training example x.    labels -- M x Dy matrix, where each row is a one-hot vector.    params -- Model parameters, these are unpacked for you.    dimensions -- A tuple of input dimension, number of hidden units                  and output dimension    """</span>    <span class="token comment" spellcheck="true"># Unpack network parameters</span>    ofs <span class="token operator">=</span> <span class="token number">0</span>    Dx<span class="token punctuation">,</span> H<span class="token punctuation">,</span> Dy <span class="token operator">=</span> <span class="token punctuation">(</span>dimensions<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dimensions<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dimensions<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    W1 <span class="token operator">=</span> np<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>params<span class="token punctuation">[</span>ofs<span class="token punctuation">:</span>ofs<span class="token operator">+</span> Dx <span class="token operator">*</span> H<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>Dx<span class="token punctuation">,</span> H<span class="token punctuation">)</span><span class="token punctuation">)</span>    ofs <span class="token operator">+=</span> Dx <span class="token operator">*</span> H    b1 <span class="token operator">=</span> np<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>params<span class="token punctuation">[</span>ofs<span class="token punctuation">:</span>ofs <span class="token operator">+</span> H<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> H<span class="token punctuation">)</span><span class="token punctuation">)</span>    ofs <span class="token operator">+=</span> H    W2 <span class="token operator">=</span> np<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>params<span class="token punctuation">[</span>ofs<span class="token punctuation">:</span>ofs <span class="token operator">+</span> H <span class="token operator">*</span> Dy<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>H<span class="token punctuation">,</span> Dy<span class="token punctuation">)</span><span class="token punctuation">)</span>    ofs <span class="token operator">+=</span> H <span class="token operator">*</span> Dy    b2 <span class="token operator">=</span> np<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>params<span class="token punctuation">[</span>ofs<span class="token punctuation">:</span>ofs <span class="token operator">+</span> Dy<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> Dy<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># Note: compute cost based on `sum` not `mean`.</span>    <span class="token comment" spellcheck="true"># forward propagation</span>    h <span class="token operator">=</span> sigmoid<span class="token punctuation">(</span>X<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>W1<span class="token punctuation">)</span> <span class="token operator">+</span> b1<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># N x H</span>    yhat <span class="token operator">=</span> softmax<span class="token punctuation">(</span>h<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>W2<span class="token punctuation">)</span> <span class="token operator">+</span> b2<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># N x Dy</span>    cost <span class="token operator">=</span> <span class="token operator">-</span>np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>np<span class="token punctuation">.</span>log<span class="token punctuation">(</span>yhat<span class="token punctuation">[</span>np<span class="token punctuation">.</span>where<span class="token punctuation">(</span>labels<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># backward propagation</span>    gradz2 <span class="token operator">=</span> <span class="token punctuation">(</span>yhat <span class="token operator">-</span> labels<span class="token punctuation">)</span>    gradW2 <span class="token operator">=</span> <span class="token punctuation">(</span>h<span class="token punctuation">.</span>T<span class="token punctuation">)</span><span class="token punctuation">.</span>dot<span class="token punctuation">(</span>gradz2<span class="token punctuation">)</span>    gradb2 <span class="token operator">=</span> np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>gradz2<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> keepdims<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    gradh <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>gradz2<span class="token punctuation">,</span> W2<span class="token punctuation">.</span>T<span class="token punctuation">)</span>    gradz1 <span class="token operator">=</span> sigmoid_grad<span class="token punctuation">(</span>h<span class="token punctuation">)</span> <span class="token operator">*</span> gradh    gradW1 <span class="token operator">=</span> <span class="token punctuation">(</span>X<span class="token punctuation">.</span>T<span class="token punctuation">)</span><span class="token punctuation">.</span>dot<span class="token punctuation">(</span>gradz1<span class="token punctuation">)</span>    gradb1 <span class="token operator">=</span> np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>gradz1<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> keepdims<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># Stack gradients</span>    grad <span class="token operator">=</span> np<span class="token punctuation">.</span>concatenate<span class="token punctuation">(</span><span class="token punctuation">(</span>gradW1<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> gradb1<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        gradW2<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> gradb2<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> cost<span class="token punctuation">,</span> grad</code></pre><h2 id="3-word2vec"><a href="#3-word2vec" class="headerlink" title="3. word2vec"></a>3. word2vec</h2><blockquote><p>(a) Assume you are given a predicted word vector $v_c$  corresponding to the center word $c$ for<br>and word prediction is made with the softmax function found in word2vec models<br>$$<br>\hat{\boldsymbol{y}}_{o}=p(\boldsymbol{o} | \boldsymbol{c})=\frac{\exp \left(\boldsymbol{u}_{o}^{\top} \boldsymbol{v}_{c}\right)}{\sum_{w=1}^{W} \exp \left(\boldsymbol{u}_{w}^{\top} \boldsymbol{v}_{c}\right)} <br>$$<br>where w denotes the $w$-th word and $u_w$ ($w = 1, \ldots, W $) are the “output” word vectors for all words in<br>the vocabulary. Assume cross entropy cost is applied to this prediction and word o is the expected word<br>(the $o$-th element of the one-hot label vector is one), derive the gradients with respect to $v_c$.</p></blockquote><p>令 $y$ 为 $Uv_c$ softmax输出的向量，$\hat{y}$ 为标签向量，形式为one-hot vector。则:</p>$$\frac{\partial J}{\partial \boldsymbol{v}_{c}}=U(\hat{\boldsymbol{y}}-\boldsymbol{y})$$或者其经过分配律后的式子：$$\frac{\partial J}{\partial \boldsymbol{v}_{c}}=-\boldsymbol{u}_{i}+\sum_{w=1}^{W} \hat{y}_{w} \boldsymbol{u}_{w}$$<blockquote><p>(b) As in the previous part, derive gradients for the “output” word vectors $u_w$’s (including $u_o$).</p></blockquote><p>与(a)基本完全一致：</p>$$\frac{\partial J}{\partial \boldsymbol{U}}=\boldsymbol{v}_{c}(\hat{\boldsymbol{y}}-\boldsymbol{y})^{\top}$$<blockquote><p>(c) Repeat part (a) and (b) assuming we are using the negative sampling loss for the predicted vector $v_c$, and the expected output word is $o$. Assume that K negative samples (words) are drawn, and<br>they are $1,\ldots, K$, respectively for simplicity of notation ($o \notin\{1, \ldots, K\}$ ). Again, for a given word, $o$,<br>denote its output vector as $u_o$. The negative sampling loss function in this case is<br>$$<br> J_{n e g-s a m p l e}\left(\boldsymbol{o}, \boldsymbol{v}_{c}, \boldsymbol{U}\right)=-\log \left(\sigma\left(\boldsymbol{u}_{o}^{\top} \boldsymbol{v}_{c}\right)\right)-\sum_{k=1}^{K} \log \left(\sigma\left(-\boldsymbol{u}_{k}^{\top} \boldsymbol{v}_{c}\right)\right) <br>$$</p></blockquote>$$\begin{aligned} \frac{\partial J}{\partial \boldsymbol{v}_{c}} &=\left(\sigma\left(\boldsymbol{u}_{o}^{\top} v_{c}\right)-1\right) \boldsymbol{u}_{o}-\sum_{k=1}^{K}\left(\sigma\left(-\boldsymbol{u}_{k}^{\top} \boldsymbol{v}_{c}\right)-1\right) \boldsymbol{u}_{k} \\ \frac{\partial J}{\partial \boldsymbol{u}_{o}} &=\left(\sigma\left(\boldsymbol{u}_{o}^{\top} \boldsymbol{v}_{c}\right)-1\right) \boldsymbol{v}_{c} \\ \frac{\partial J}{\partial \boldsymbol{u}_{k}} &=-\left(\sigma\left(-\boldsymbol{u}_{k}^{\top} \boldsymbol{v}_{c}\right)-1\right) \boldsymbol{v}_{c}, \quad \text { for all } k=1,2, \ldots, K \end{aligned}$$<blockquote><p>(d) Derive gradients for all of the word vectors for skip-gram and CBOW given the previous parts and given a set of context words  $[word_{c-m}, \ldots, word_{c-1}; word_c; word_{c+1}, \ldots word_{c+m}] $, where m is the context size. Denote the “input” and “output” word vectors for $word_k$ as $v_k$ and $u_k$ respectively.</p></blockquote><p>这里不妨使用$F(o, v_c)$ 代表  $J_{softmax-CE}(o, v_c, \ldots)$ 或 $J_{neg-sample}(o,v_c, \ldots)$ 。</p>对Skip-gram：$$\begin{array}{l}{\dfrac{\partial J_{\text {skip-gram }}\left(\text { word }_{c-m \ldots c+m}\right)}{\partial U}=\sum_{-m \leq j \leq m, j \neq 0} \dfrac{\partial F\left(\boldsymbol{w}_{c+j}, \boldsymbol{v}_{c}\right)}{\partial \boldsymbol{U}}} \\ {\dfrac{\partial J_{\text {skip-gram }}\left(\text { word }_{c-m \ldots c+m}\right)}{\partial \boldsymbol{v}_{c}}=\sum_{-m \leq j \leq m, j \neq 0} \dfrac{\partial F\left(\boldsymbol{w}_{c+j}, \boldsymbol{v}_{c}\right)}{\partial \boldsymbol{v}_{c}}} \\ {\dfrac{\partial J_{\text {skip-gram }}\left(\text { word }_{c-m \ldots c+m}\right)}{\partial \boldsymbol{v}_{j}}=0, \text { for all } j \neq c}\end{array}$$对CBOW：$$\begin{array}{l}{\dfrac{\partial J_{\mathrm{CBOW}}\left(\text { word }_{c-m \ldots c+m}\right)}{\partial U}=\dfrac{\partial F\left(\boldsymbol{w}_{c}, \hat{\boldsymbol{v}}\right)}{\partial \boldsymbol{U}}=\dfrac{\partial F\left(\boldsymbol{w}_{c}, \hat{\boldsymbol{v}}\right)}{\partial \boldsymbol{U}}, \quad \text { (using the definition of } \hat{\boldsymbol{v}} \text { in the problem) }} \\ {\dfrac{\partial J_{\mathrm{CBOW}}\left(\text { word }_{c-m, \ldots c+m}\right)}{\partial \boldsymbol{v}_{j}}=\dfrac{\partial F\left(\boldsymbol{w}_{c}, \hat{\boldsymbol{v}}\right)}{\partial \hat{\boldsymbol{v}}}, \quad \text { for all } j \in\{c-m, \ldots, c-1, c+1, \ldots, c+m\}} \\ {\dfrac{\partial J_{\mathrm{CBOW}}\left(\text { word }_{c-m, \ldots c+m}\right)}{\partial \boldsymbol{v}_{j}}=\mathbf{0}, \quad \text { for all } j \notin\{c-m, \ldots, c-1, c+1, \ldots, c+m\}}\end{array}$$]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Word2vec-Summary&quot;&gt;&lt;a href=&quot;#Word2vec-Summary&quot; class=&quot;headerlink&quot; title=&quot;Word2vec Summary&quot;&gt;&lt;/a&gt;Word2vec Summary&lt;/h2&gt;&lt;h3 id=&quot;背景&quot;&gt;&lt;a hr</summary>
      
    
    
    
    <category term="NLP" scheme="https://huangweiran.club/categories/NLP/"/>
    
    
    <category term="NLP" scheme="https://huangweiran.club/tags/NLP/"/>
    
    <category term="Deep Learning" scheme="https://huangweiran.club/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>翻译：如何在Jupyter notebook中安装Python包？</title>
    <link href="https://huangweiran.club/2018/05/14/%E7%BF%BB%E8%AF%91%EF%BC%9A%E5%A6%82%E4%BD%95%E5%9C%A8Jupyter-notebook%E4%B8%AD%E5%AE%89%E8%A3%85Python%E5%8C%85%EF%BC%9F/"/>
    <id>https://huangweiran.club/2018/05/14/%E7%BF%BB%E8%AF%91%EF%BC%9A%E5%A6%82%E4%BD%95%E5%9C%A8Jupyter-notebook%E4%B8%AD%E5%AE%89%E8%A3%85Python%E5%8C%85%EF%BC%9F/</id>
    <published>2018-05-14T19:40:16.000Z</published>
    <updated>2021-06-29T07:55:20.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Installing-Python-Packages-from-a-Jupyter-Notebook"><a href="#Installing-Python-Packages-from-a-Jupyter-Notebook" class="headerlink" title="Installing Python Packages from a Jupyter Notebook"></a>Installing Python Packages from a Jupyter Notebook</h1><h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p><em>This notebook originally appeared as a <a href="http://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/" target="_blank" rel="noopener">post</a> on the blog <a href="http://jakevdp.github.io" target="_blank" rel="noopener">Pythonic Perambulations</a>.</em></p><p>本文由我完成翻译，以帮助更多中国朋友解决这个问题。但由于知识和能力的限制，译文很可能存在错误和纰漏，造成对部分朋友的误导。因此如果你有什么意见或建议，欢迎联系我以完善译文。如果我的译文使你产生了困惑，请参考上述原文。</p><p>一些额外的说明：</p><ol><li>考虑到它们在计算机领域的常见性，文中除了首次出现的地方，kernel和shell一律使用英文；</li><li>考虑到Jupyter notebook的名称，文中的notebook一律不翻译；</li><li>文中的小标题都没有翻译。</li></ol><h2 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h2><!-- PELICAN_BEGIN_SUMMARY --><p>在软件开发领域，有句话叫做<a href="https://www.joelonsoftware.com/2002/11/11/the-law-of-leaky-abstractions/" target="_blank" rel="noopener">一切抽象都是有漏洞的</a>（译者注：这是指在软件开发过程中，本应隐藏实现细节的抽象化不可避免地暴露出底层细节与局限性。抽象泄露是棘手的问题，因为抽象化本来目的就是向用户隐藏不必要公开的细节），如同其他软件一样，这句话对Jupyter notebook也同样适用。</p><p>下面这种情况的频繁出现证明了这一点：</p><blockquote><p>我安装了<em>XX包</em> 但是我不能在notebook中导入它，谁能帮帮我？</p></blockquote><p>这个问题多年来在StackOverflow上盛久不衰 (比如 <a href="https://stackoverflow.com/questions/39007571/running-jupyter-with-multiple-python-and-ipython-paths/" target="_blank" rel="noopener">问题A</a>, <a href="https://stackoverflow.com/questions/42500142/importerror-no-module-named-jwt-in-jupyter" target="_blank" rel="noopener">问题B</a>, 还有<a href="https://stackoverflow.com/questions/32777807/importerror-no-module-named-cv2-using-jupyter" target="_blank" rel="noopener">问题C</a>, 和<a href="https://stackoverflow.com/questions/42500649/failed-to-import-numpy-as-np-when-i-worked-with-jupyter-notebook" target="_blank" rel="noopener">问题D</a>, <a href="https://stackoverflow.com/questions/46634660/jupyter-notebook-wrong-sys-path-and-sys-executable" target="_blank" rel="noopener">问题E</a>, <a href="https://stackoverflow.com/questions/44222513/cannot-import-datashader-installed-using-miniconda" target="_blank" rel="noopener">问题F</a>, 以及<a href="https://stackoverflow.com/questions/42178070/jupyter-notebook-importerror-no-module-named-sklearn" target="_blank" rel="noopener">问题G</a>, 和<a href="https://stackoverflow.com/questions/42034508/fail-pandas-in-python3-jupyter-notebook" target="_blank" rel="noopener">问题H</a>… etc.)。</p><p>从本质上来说，这个问题通常源于这么一个事实：<strong>Jupyter的内核（kernel）和Jupyter的壳（shell）是不相连的</strong>；换句话说，安装程序指向的是另一个Python的版本，而非你在notebook中使用的那一个。<br>在最简单的情况下这种问题并不会出现，但一旦它出现了，解决这个问题就需要你具有关于操作系统、Python包的安装、以及Jupyter本身的相关知识。<br>换言之，Jupyter notebook，与其他所有的抽象化的软件一样，也存在漏洞。</p><p>在与同事们针对这个问题进行了一些讨论后——有的是线上(这是我们两次线上讨论的链接：<a href="https://twitter.com/amuellerml/status/932637063748444160" target="_blank" rel="noopener">讨论 A</a>, <a href="https://twitter.com/jakevdp/status/922846245848150016" target="_blank" rel="noopener">讨论 B</a>)， 有的是线下——我决定在这里对这个问题进行一些深入的讲解。</p><p>这篇博文旨在阐述清楚以下内容：</p><ul><li><p><strong>首先</strong>， 我会针对这个问题提供过一个快速、简单的解决办法，<em>我要怎样通过pip或者conda在我的jupyter notebook上安装一个python包呢？</em></p></li><li><p><strong>其次</strong>，我会深入这些问题的具体背景：Jupyter notebook的抽象化究竟在<em>做什么</em>， 它是如何与复杂的操作系统进行交互的，以及你要怎样看待这所谓的泄漏，从而对问题出现的前因后果有一个完整的了解。</p></li><li><p><strong>第三</strong>，我会谈一些我的想法，也许开源社区可以考虑一下这些点子以消除这个问题，这包括了Jupyter，Pip，和Conda的开发者需要做的一些改进，来减轻使用者对其产品的认知负担。</p></li></ul><p>这篇文章会专注于安装Python包的两个途径： <a href="https://pip.pypa.io/en/stable/" target="_blank" rel="noopener">pip</a> 和 <a href="https://conda.io/docs/" target="_blank" rel="noopener">conda</a>.<br>其他管理包的方式当然也存在 (包括特定平台的工具，比如 <a href="http://yum.baseurl.org/" target="_blank" rel="noopener">yum</a>, <a href="https://help.ubuntu.com/community/AptGet/Howto#Package_management_with_APT" target="_blank" rel="noopener">apt</a>, <a href="https://brew.sh/" target="_blank" rel="noopener">homebrew</a>, etc., 以及跨平台的工具 <a href="http://enstaller.readthedocs.io/en/latest/" target="_blank" rel="noopener">enstaller</a>)，但我对它们并不那么熟悉，因此不会再对他们进行深入的讨论。</p><!-- PELICAN_END_SUMMARY --><h2 id="Quick-Fix-How-To-Install-Packages-from-the-Jupyter-Notebook"><a href="#Quick-Fix-How-To-Install-Packages-from-the-Jupyter-Notebook" class="headerlink" title="Quick Fix: How To Install Packages from the Jupyter Notebook"></a>Quick Fix: How To Install Packages from the Jupyter Notebook</h2><p>如果你只是想快速解决这个问题，即<em>如何在notebook里安装一个包</em>，那么只看这里就够了。</p><h3 id="pip-vs-conda"><a href="#pip-vs-conda" class="headerlink" title="pip vs. conda"></a>pip vs. conda</h3><p>首先，我简单提一下<code>pip</code> 还是 <code>conda</code>的问题。</p><p>对很多使用者来说，选择pip还是conda是非常令人困惑的问题。</p><p>去年，针对这个问题我写了<a href="https://jakevdp.github.io/blog/2016/08/25/conda-myths-and-misconceptions/" target="_blank" rel="noopener">一篇详尽得可能超过你想象的博文</a> , 但这二者的本质区别可以这样概括:</p><ul><li><em>pip</em> 可以在<strong>所有环境下</strong>安装<strong>python</strong>包。</li><li><em>conda</em>可以在<strong>conda环境下</strong>安装<strong>所有</strong>包。</li></ul><p>如果你已经安装了python，那么这个选择对你来说是非常容易的：</p><ul><li><p>如果你是用Anaconda或者Miniconda安装的python，那么请使用<code>conda</code>命令来安装python包。如果conda告诉你你要下载的这个包不存在，那么使用pip<br>(或者试试 <a href="https://conda-forge.org/" target="_blank" rel="noopener">conda-forge</a>, 它比conda默认的包的数量要更多)。</p></li><li><p>如果你是使用其他方式安装的python，那么使用<code>pip</code>来安装python包。</p></li></ul><p>最后，因为这样的问题常常发生，我必须提醒你<strong>永远不要</strong>使用<code>sudo pip install</code>。</p><p><em>永远不要。</em></p><p>即便这么做在短期内看起来好像解决了问题，但它总是会在更长的时间范围下带来问题。<br>比如，如果<code>pip install</code>给了你一个许可错误（permission error），这很可能意味着你在试图安装/升级系统python中的包，比如<code>usr/bin/python</code>。这么做会带来糟糕的后果，因为通常操作系统本身就依赖着某种特定版本的包。<br>对于日常的python使用，你应该把你的包和系统python的隔离开来，使用 <a href="https://virtualenv.pypa.io/en/stable/" target="_blank" rel="noopener">虚拟环境</a> or <a href="https://conda.io/docs/user-guide/install/download.html" target="_blank" rel="noopener">Anaconda/Miniconda</a> — 这种场合下我比较喜欢用conda，但我也知道我的很多同事会偏爱virtualenv。</p><h3 id="How-to-use-Conda-from-the-Jupyter-Notebook"><a href="#How-to-use-Conda-from-the-Jupyter-Notebook" class="headerlink" title="How to use Conda from the Jupyter Notebook"></a>How to use Conda from the Jupyter Notebook</h3><p>如果你在使用jupyter notebook并且你希望通过conda来安装某个包，你也许会想要使用<code>!</code>来直接在notebook里以shell的方式运行conda：</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># DON'T DO THIS!</span>!conda install <span class="token operator">-</span><span class="token operator">-</span>yes numpy</code></pre><pre><code>Fetching package metadata ...........Solving package specifications: .# All requested packages already installed.# packages in environment at /Users/jakevdp/anaconda/envs/python3.6:#numpy                     1.13.3           py36h2cdce51_0  </code></pre><p>(注意：当conda请求用户确认时，我们使用 <code>--yes</code> 来自动回答 <code>y</code> )</p><p>出于多种原因（这些原因我将在下面详细介绍），总之，如果你在当前的notebook中使用这样安装的包，这种操作<strong>通常来说不会起作用</strong>，尽管在很简单的情况下它会起作用。</p><p>而以下才是通用的安装方式：</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Install a conda package in the current Jupyter kernel</span><span class="token keyword">import</span> sys!conda install <span class="token operator">-</span><span class="token operator">-</span>yes <span class="token operator">-</span><span class="token operator">-</span>prefix <span class="token punctuation">{</span>sys<span class="token punctuation">.</span>prefix<span class="token punctuation">}</span> numpy</code></pre><pre><code>Fetching package metadata ...........Solving package specifications: .# All requested packages already installed.# packages in environment at /Users/jakevdp/anaconda:#numpy                     1.13.3           py36h2cdce51_0  </code></pre><p>这些增加的内容确保了conda能在当前运行的Jupyter核内安装这个包。 (感谢 <a href="https://twitter.com/minrk/status/842067777150169088" target="_blank" rel="noopener">Min Ragan-Kelley</a> 提出了这个办法).</p><p>稍后我将讨论为什么这么做是必须的。</p><h3 id="How-to-use-Pip-from-the-Jupyter-Notebook"><a href="#How-to-use-Pip-from-the-Jupyter-Notebook" class="headerlink" title="How to use Pip from the Jupyter Notebook"></a>How to use Pip from the Jupyter Notebook</h3><p>如果你在使用Jupyter notebook并且希望通过<code>pip</code>来安装一个包，与上面类似，你也许同样会想用如下方式在shell里直接运行pip：</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># DON'T DO THIS</span>!pip install numpy</code></pre><pre><code>Requirement already satisfied: numpy in /Users/jakevdp/anaconda/envs/python3.6/lib/python3.6/site-packages</code></pre><p>出于多种原因（这些原因我将在下面详细介绍），总之，如果你在当前的notebook中使用这样安装的包，这种操作<strong>通常来说不会起作用</strong>，尽管在很简单的情况下它会起作用。</p><p>而以下才是通用的安装方式：</p><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Install a pip package in the current Jupyter kernel</span><span class="token keyword">import</span> sys!<span class="token punctuation">{</span>sys<span class="token punctuation">.</span>executable<span class="token punctuation">}</span> <span class="token operator">-</span>m pip install numpy</code></pre><pre><code>Requirement already satisfied: numpy in /Users/jakevdp/anaconda/lib/python3.6/site-packages</code></pre><p>增加的内容确保了你在使用与当前Python核相关的<code>pip</code>版本，因此你所安装的包能在当前notebook中使用。<br>这实际上源于如下这样一个事实，即时不考虑Jupyter notebook，使用</p><pre><code>$ python -m pip install &lt;package&gt;</code></pre><p>来安装一个包也要比</p><pre><code>$ pip install &lt;package&gt;</code></pre><p>更好。</p><p>因为前者更清晰地指明了这个包将安装的位置（这一点之后也会详细阐述）。</p><h2 id="The-Details-Why-is-Installation-from-Jupyter-so-Messy"><a href="#The-Details-Why-is-Installation-from-Jupyter-so-Messy" class="headerlink" title="The Details: Why is Installation from Jupyter so Messy?"></a>The Details: Why is Installation from Jupyter so Messy?</h2><p>上面这些解决办法应该能够应对所有情况……但是为什么我们非得这么做？<br>简单来说，这是因为在Jupyter里，<strong>shell环境和python可执行文件是分离的</strong>。<br>想要理解为什么这一点造成了我们现在面对的情况需要你对如下几个不同概念有一个基本的了解：</p><ol><li>你的操作系统是如何定位可执行程序的，</li><li>Python是如何安装并定位包的，</li><li>Jupyter是如何确定使用哪一个Python可执行文件的。</li></ol><p>为求完整性，我将对上述每个问题做一些简单的探究（这里的讨论部分源于<br>我去年写的 <a href="https://stackoverflow.com/questions/39007571/running-jupyter-with-multiple-python-and-ipython-paths/39022003#39022003" target="_blank" rel="noopener">这个StackOverflow上的回答</a> ).</p><p><em>注意：以下讨论基于Linux，Unix，MacOSX和其他类似的操作系统。Windows有一个略微不同的体系结构，因此在一些细节上会存在差异。</em></p><h3 id="How-your-operating-system-locates-executables"><a href="#How-your-operating-system-locates-executables" class="headerlink" title="How your operating system locates executables"></a>How your operating system locates executables</h3><p>当你在终端输入诸如 <code>python</code>, <code>jupyter</code>, <code>ipython</code>, <code>pip</code>, <code>conda</code> 这样的命令时，你的操作系统会根据它所具有的一套定义明确的机制来找到这些命令所对应的可执行文件。</p><p>在Linux &amp; Mac操作系统中，系统会首先寻找是否有一个<a href="http://tldp.org/LDP/abs/html/aliases.html" target="_blank" rel="noopener">别名（alias）</a>与这个命令匹配；如果它没能在<code>$PATH$</code>环境变量中找到：</p><pre class=" language-python"><code class="language-python">!echo $PATH</code></pre><pre><code>/Users/jakevdp/anaconda/envs/python3.6/bin:/Users/jakevdp/anaconda/envs/python3.6/bin:/Users/jakevdp/anaconda/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin</code></pre><p><code>$PATH</code>中罗列了用来查找可执行文件的目录：举例来说，如果我在我的系统上用上面的<code>$PATH</code>输入<code>python</code>，它首先会寻找<code>/Users/jakevdp/anaconda/envs/python3.6/bin/python</code>，如果这个不存在，那么它会寻找<code>/Users/jakevdp/anaconda/bin/python</code>等等。</p><p>（顺带说明：为什么<code>$PATH</code>的第一个条目重复了两次？这是因为每次你打开<code>jupyter notebook</code>的时候，Jupyter会把<code>jupyter</code>可执行文件的地址放到<code>$PATH</code>的开头。在我们的例子中，这个地址已经在路径的开头了，于是结果就是这个条目将会重复。重复的条目可能会造成一些混乱，但也没什么坏处）。</p><p>如果你想知道当你输入<code>python</code>的时候，究竟是哪个文件被执行，你可以使用<code>type</code>shell命令：</p><pre class=" language-python"><code class="language-python">!type python</code></pre><pre><code>python is /Users/jakevdp/anaconda/envs/python3.6/bin/python</code></pre><p>注意，这对你在终端中使用的<em>任何</em>命令都适用：</p><pre class=" language-python"><code class="language-python">!type ls</code></pre><pre><code>ls is /bin/ls</code></pre><p>即使是内置的命令，比如<code>type</code>本身：</p><pre class=" language-python"><code class="language-python">!type type</code></pre><pre><code>type is a shell builtin</code></pre><p>你可以选择增加一个<code>-a</code>标签，来查看指令的<em>所有</em>可执行的版本；比如：</p><pre class=" language-python"><code class="language-python">!type <span class="token operator">-</span>a python</code></pre><pre><code>python is /Users/jakevdp/anaconda/envs/python3.6/bin/pythonpython is /Users/jakevdp/anaconda/envs/python3.6/bin/pythonpython is /Users/jakevdp/anaconda/bin/pythonpython is /usr/bin/python</code></pre><pre class=" language-python"><code class="language-python">!type <span class="token operator">-</span>a conda</code></pre><pre class=" language-shell"><code class="language-shell">conda is /Users/jakevdp/anaconda/envs/python3.6/bin/condaconda is /Users/jakevdp/anaconda/envs/python3.6/bin/condaconda is /Users/jakevdp/anaconda/bin/conda</code></pre><pre class=" language-python"><code class="language-python">!type <span class="token operator">-</span>a pip</code></pre><pre><code>pip is /Users/jakevdp/anaconda/envs/python3.6/bin/pippip is /Users/jakevdp/anaconda/envs/python3.6/bin/pippip is /Users/jakevdp/anaconda/bin/pip</code></pre><p>当你有一个命令的多个不同版本时，了解<code>$PATH</code>在选择“究竟哪一个将被使用”上的重要角色将尤为重要。</p><h3 id="How-Python-locates-packages"><a href="#How-Python-locates-packages" class="headerlink" title="How Python locates packages"></a>How Python locates packages</h3><p>Python使用一个类似的机制来定位导入的包。<br>Python搜索的路径列表可在<code>sys.path</code>中找到：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> syssys<span class="token punctuation">.</span>path</code></pre><pre><code>[&#39;&#39;, &#39;/Users/jakevdp/anaconda/lib/python36.zip&#39;, &#39;/Users/jakevdp/anaconda/lib/python3.6&#39;, &#39;/Users/jakevdp/anaconda/lib/python3.6/lib-dynload&#39;, &#39;/Users/jakevdp/anaconda/lib/python3.6/site-packages&#39;, &#39;/Users/jakevdp/anaconda/lib/python3.6/site-packages/schemapi-0.3.0.dev0+791c7f6-py3.6.egg&#39;, &#39;/Users/jakevdp/anaconda/lib/python3.6/site-packages/setuptools-27.2.0-py3.6.egg&#39;, &#39;/Users/jakevdp/anaconda/lib/python3.6/site-packages/IPython/extensions&#39;, &#39;/Users/jakevdp/.ipython&#39;]</code></pre><p>默认情况下，Python搜索模块的第一个位置是一个空路径，代表当前工作目录。</p><p>如果在这个位置该模块没有被找到，那么它会沿列表向下直到找到为止。</p><p>你可以通过导入的模块的<code>__path__</code>属性来获知究竟是哪个地址正在被使用：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> numpynumpy<span class="token punctuation">.</span>__path__</code></pre><pre><code>[&#39;/Users/jakevdp/anaconda/lib/python3.6/site-packages/numpy&#39;]</code></pre><p>在大多数情况下，你使用<code>pip</code>或者<code>conda</code>安装的Python包会存放在一个名为<code>site-packages</code>的目录下。重要的一点是，每个Python可执行文件都有<strong>它自己的<code>site-packages</code></strong>：这就意味着，当你安装一个包时，它会与某一个特定的Python可执行文件相关联，并且在默认情况下只能在这个Python版本下使用！</p><p>我们可以通过对我的每一个可用的<code>python</code>来打印其<code>sys.path</code>来验证这一点，这得益于Jupyter的一个令人振奋的能力，那就是能够在一个代码块中把Python和bash命令结合：</p><pre class=" language-python"><code class="language-python">paths <span class="token operator">=</span> !type <span class="token operator">-</span>a python<span class="token keyword">for</span> path <span class="token keyword">in</span> set<span class="token punctuation">(</span>paths<span class="token punctuation">)</span><span class="token punctuation">:</span>    path <span class="token operator">=</span> path<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>path<span class="token punctuation">)</span>    !<span class="token punctuation">{</span>path<span class="token punctuation">}</span> <span class="token operator">-</span>c <span class="token string">"import sys; print(sys.path)"</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre><pre><code>/Users/jakevdp/anaconda/envs/python3.6/bin/python[&#39;&#39;, &#39;/Users/jakevdp/anaconda/envs/python3.6/lib/python36.zip&#39;, &#39;/Users/jakevdp/anaconda/envs/python3.6/lib/python3.6&#39;, &#39;/Users/jakevdp/anaconda/envs/python3.6/lib/python3.6/lib-dynload&#39;, &#39;/Users/jakevdp/anaconda/envs/python3.6/lib/python3.6/site-packages&#39;]/usr/bin/python[&#39;&#39;, &#39;/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python27.zip&#39;, &#39;/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7&#39;, &#39;/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/plat-darwin&#39;, &#39;/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/plat-mac&#39;, &#39;/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/plat-mac/lib-scriptpackages&#39;, &#39;/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-tk&#39;, &#39;/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-old&#39;, &#39;/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/lib-dynload&#39;, &#39;/Library/Python/2.7/site-packages&#39;, &#39;/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python&#39;, &#39;/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/PyObjC&#39;]/Users/jakevdp/anaconda/bin/python[&#39;&#39;, &#39;/Users/jakevdp/anaconda/lib/python36.zip&#39;, &#39;/Users/jakevdp/anaconda/lib/python3.6&#39;, &#39;/Users/jakevdp/anaconda/lib/python3.6/lib-dynload&#39;, &#39;/Users/jakevdp/anaconda/lib/python3.6/site-packages&#39;, &#39;/Users/jakevdp/anaconda/lib/python3.6/site-packages/schemapi-0.3.0.dev0+791c7f6-py3.6.egg&#39;, &#39;/Users/jakevdp/anaconda/lib/python3.6/site-packages/setuptools-27.2.0-py3.6.egg&#39;]</code></pre><p>​    </p><p>这里的具体细节并不十分重要，但你必须知道<em>每一个Python可执行文件都有其独有的路径</em>，并且除非你修改了<code>sys.path</code>（如果要这么做你必须格外小心）你是不可能在另一个Python环境中导入包的。</p><p>当你使用<code>pip install</code>或者<code>conda install</code>的时候，这些命令与具体的Python版本紧密相关：</p><ul><li><code>pip</code>为同一路径下的Python安装包</li><li><code>conda</code>为当前是激活状态的conda环境安装包</li></ul><p>因此，我们可以看到，下面这个<code>pip install</code>会为这个名为<code>python3.6</code>的conda环境执行安装操作：</p><pre class=" language-python"><code class="language-python">!type pip</code></pre><pre><code>pip is /Users/jakevdp/anaconda/envs/python3.6/bin/pip</code></pre><p>与此同时，<code>conda install</code>也能做同样的事情，因为<code>python3.6</code>正是当前激活状态下的环境（注意<code>*</code>代表被激活的环境）：</p><pre class=" language-python"><code class="language-python">!conda env list</code></pre><pre><code># conda environments:#python2.7                /Users/jakevdp/anaconda/envs/python2.7python3.5                /Users/jakevdp/anaconda/envs/python3.5python3.6             *  /Users/jakevdp/anaconda/envs/python3.6rstats                   /Users/jakevdp/anaconda/envs/rstatsroot                     /Users/jakevdp/anaconda</code></pre><p>这里<code>pip</code>和<code>conda</code>都能默认为conda的<code>python3.6</code>环境安装包的原因是这个环境就是我运行notebook的环境。</p><p>我要重述一遍这句话以示强调：<strong>Jupyter notebook的shell环境与用于<em>运行</em>这个notebook的环境是匹配的。</strong></p><h3 id="How-Jupyter-executes-code-Jupyter-Kernels"><a href="#How-Jupyter-executes-code-Jupyter-Kernels" class="headerlink" title="How Jupyter executes code: Jupyter Kernels"></a>How Jupyter executes code: Jupyter Kernels</h3><p>下一个相关的问题就是，Jupyter怎样执行Python代码。这个问题把我们引向了<em>Jupyter kernel</em>的概念。</p><p>一个Jupyter kernel是这样一组文件：它们能引导Jupyter找到在notebook中执行代码的方法。<br>对于Python kernel，他们会指向一个特定的Python版本，但Jupyter的能力比这要更强大：Jupyter具有针对多种语言的<a href="https://github.com/jupyter/jupyter/wiki/Jupyter-kernels" target="_blank" rel="noopener">许多可用的kernel</a> ，这些语言包括Python2， Python3，Julia， R，Ruby，Haskell，甚至C++和Fortran！</p><pre class=" language-python"><code class="language-python">!jupyter kernelspec list</code></pre><pre><code>Available kernels:  python3       /Users/jakevdp/anaconda/envs/python3.6/lib/python3.6/site-packages/ipykernel/resources  conda-root    /Users/jakevdp/Library/Jupyter/kernels/conda-root  python2.7     /Users/jakevdp/Library/Jupyter/kernels/python2.7  python3.5     /Users/jakevdp/Library/Jupyter/kernels/python3.5  python3.6     /Users/jakevdp/Library/Jupyter/kernels/python3.6</code></pre><p>上面罗列的每一个kernel都是一个目录，在这些目录下包含了一个叫做<code>kernel.json</code>的文件，文件中制指定了这个kernel应该使用哪种语言和可执行文件。<br>比如：</p><pre class=" language-python"><code class="language-python">!cat <span class="token operator">/</span>Users<span class="token operator">/</span>jakevdp<span class="token operator">/</span>Library<span class="token operator">/</span>Jupyter<span class="token operator">/</span>kernels<span class="token operator">/</span>conda<span class="token operator">-</span>root<span class="token operator">/</span>kernel<span class="token punctuation">.</span>json</code></pre><pre><code>{ &quot;argv&quot;: [  &quot;/Users/jakevdp/anaconda/bin/python&quot;,  &quot;-m&quot;,  &quot;ipykernel_launcher&quot;,  &quot;-f&quot;,  &quot;{connection_file}&quot; ], &quot;display_name&quot;: &quot;python (conda-root)&quot;, &quot;language&quot;: &quot;python&quot;}</code></pre><p>如果你想要创建一个新的kernel，你可以使用<a href="http://ipython.readthedocs.io/en/stable/install/kernel_install.html#kernels-for-different-environments" target="_blank" rel="noopener">jupyter ipykernel命令</a>；<br>举个例子，我使用如下命令为模板为我主要的conda环境创建上述kernel:</p><pre><code>$ source activate myenv$ python -m ipykernel install --user --name myenv --display-name &quot;Python (myenv)&quot;</code></pre><h2 id="The-Root-of-the-Issue"><a href="#The-Root-of-the-Issue" class="headerlink" title="The Root of the Issue"></a>The Root of the Issue</h2><p>现在我们已经有了足够的背景知识来回答这个问题：*为什么使用<code>!pip install</code>或者<code>!conda install</code>通常都对notebook不奏效？</p><p>问题的根源是：当Jupyter notebook开始运行，shell环境就已经被确定了，然而Python的可执行文件却要由kernel来决定，而这二者并不一定互相匹配。<br>换言之，你并不能保证你<code>$PATH</code>中的<code>python</code>，<code>pip</code>和<code>conda</code>会和notebook中的<code>python</code>相适配。</p><p>回忆一下，你路径下的<code>python</code>可以通过如下命令来确定：</p><pre class=" language-python"><code class="language-python">!type python</code></pre><pre><code>python is /Users/jakevdp/anaconda/envs/python3.6/bin/python</code></pre><p>notebook中的Python可执行文件则可以通过如下命令来确定：</p><pre class=" language-python"><code class="language-python">sys<span class="token punctuation">.</span>executable</code></pre><pre><code>&#39;/Users/jakevdp/anaconda/bin/python&#39;</code></pre><p>在我当前的notebook环境中，这两个Python并不相同。<br>这就是为什么简单的<code>!pip install</code>或者<code>!conda install</code>会不奏效：这两个命令会把包安在错误的Python的<code>site-packages</code>下。</p><p>如我们上面所说，我们可以通过明确确定安装位置来克服这个问题。<br>在<strong>conda</strong>中，你可以手动在shell命令中这样增加前缀：</p><pre><code>$ conda install --yes --prefix /Users/jakevdp/anaconda numpy</code></pre><p>或者，为了能够自动使用正确的前缀（使用notebook中可用的语法）：</p><pre><code>!conda install --yes --prefix {sys.prefix} numpy</code></pre><p>对于<strong>pip</strong>，你可以明确指出你需要的Python版本:</p><pre><code>$ /Users/jakevdp/anaconda/bin/python -m pip install numpy</code></pre><p>或者，为了能够自动使用正确版本（又一次使用notebook的shell语法）：</p><pre><code>!{sys.executable} -m pip install numpy</code></pre><p>记住这一点：如果你想安装能在当前notebook中使用的包，你需要你的<em>安装命令</em>与<em>当前Python的kernel</em>相匹配。</p><h2 id="Some-Modest-Proposals"><a href="#Some-Modest-Proposals" class="headerlink" title="Some Modest Proposals"></a>Some Modest Proposals</h2><p>因此，总括地说，在Jupyter notebook中安装包之所以困难重重，是因为<strong>Jupyter的shell和Python的kernel不匹配</strong>，这就意味着比起直接<code>pip install</code>或者<code>conda install</code>，你还得做点别的什么。<br>之前我们提到的特殊情况下的例外，就是指你刚好在kernel所指的同一个Python环境下运行<code>jupyter notebook</code>；在这种情况下，简单的安装命令就会奏效。</p><p>尽管我们解决了这个问题，但这个问题仍然把我们置于一个不太好的环境下，因为对于一个新手而言，他会认为这样的事情应该简单如此：安装一个包，然后使用，仅此而已。而这个情况增大了他们的学习曲线，使得Jupyter显得不那么易用。<br>那么如果我们是开发社区的成员，我们要怎么做才能消除这个问题呢？</p><p>我有一些点子，我想其中的一些会有用的：</p><h3 id="Potential-Changes-to-Jupyter"><a href="#Potential-Changes-to-Jupyter" class="headerlink" title="Potential Changes to Jupyter"></a>Potential Changes to Jupyter</h3><p>如我所说，问题根源在于Jupyter的shell环境和kernel的不匹配。<br>那么，我们能否为kernel规范发送信号以强制使二者匹配？<br>So, could we massage kernel specifications such that they force the two to match?</p><p>也许可以：比如，<a href="https://github.com/jupyterhub/jupyterhub/issues/847" target="_blank" rel="noopener">这个github issue</a> 向我们展示了一种修改shell变量，使它成为kernel启动的一部分的方法。</p><p>简单来说，在你的kernel目录中，你可以增加一个脚本<code>kernel-startup.sh</code>，它的内容大致如下（注意修改权限以使之可执行）：</p><pre><code>#!/usr/bin/env bash# activate anaconda envsource activate myenv# this is the critical part, and should be at the end of your script:exec python -m ipykernel $@</code></pre><p>之后在你的<code>kernel.json</code>文件中，修改<code>argv</code>的内容如下：</p><pre><code>&quot;argv&quot;: [   &quot;/path/to/kernel-startup.sh&quot;,   &quot;-f&quot;,   &quot;{connection_file}&quot; ]</code></pre><p>一旦你做了这些，转入<code>myenv</code>的kernel会自动激活<code>myenv</code>的conda环境，这会改变你的 <code>$CONDA_PREFIX</code>，<code>$PATH</code> 以及其他的系统变量，这会使<code>!conda install XXX</code> 和 <code>!pip install XXX</code> 能够正确工作。在virtualenvs或者其他Python环境中也可以使用类似的方法。</p><p>这里有一个麻烦的问题：如果你的<code>myenv</code>环境下没有<code>ipykernel</code>包，也许还得有与之匹配的用于打开notebook的jupyter版本，这个方法将不会有用。因此这不是一个完美的解决办法，不过如果Python kernels能够被设定为可以默认执行这样的shell初始化工作，对用户而言这就不会那么让人困惑了：<code>!pip install</code>和<code>!conda install</code>就够了。</p><h3 id="Potential-Changes-to-pip"><a href="#Potential-Changes-to-pip" class="headerlink" title="Potential Changes to pip"></a>Potential Changes to pip</h3><p>即便在Jupyter之外，关于安装的困惑也并不鲜见。它的一个来源就是，由于系统别名和环境变量的特性，<code>pip</code>和<code>python</code>可能会指向不同的路径。<br>在这样的情况下，<code>pip install</code>会在一个<code>python</code>无法连接的地址安装包。<br>因此，使用<code>python -m pip install</code>会是一个更加安全的做法。它明确指定了需要安装包的Python版本(毕竟<a href="https://www.python.org/dev/peps/pep-0020/" target="_blank" rel="noopener">明确总比模糊要好</a>）。</p><p>这就是<a href="https://docs.python.org/3/installing/index.html#basic-usage" target="_blank" rel="noopener">Python文档</a>里不再有<code>pip install</code>，像David Beazley这样经验丰富的Python教学者<a href="https://twitter.com/dabeaz/status/922859605247643649" target="_blank" rel="noopener">从来不教的“裸露的pip”（bare pip)</a>的一个原因。<br>Cpython开发者Nick Colghlan甚至<a href="https://twitter.com/ncoghlan_dev/status/922979220711661568" target="_blank" rel="noopener">指出</a>总有一天<code>pip</code>会被<code>python -m pip</code>取代。<br>尽管这样做命令好像更冗长了，但我还是认为强制用户使用更明确的命令会是一个很有用的改变，尤其是在当下virtualenvs和conda正在日渐普遍的情况下。</p><h3 id="Changes-to-Conda"><a href="#Changes-to-Conda" class="headerlink" title="Changes to Conda"></a>Changes to Conda</h3><p>我认为对conda的API做一些修改会对用户来说很有帮助。</p><h4 id="Explicit-invocation"><a href="#Explicit-invocation" class="headerlink" title="Explicit invocation"></a>Explicit invocation</h4><p>与<code>pip</code>相对应，我认为使用<code>python -m conda install</code>会与上面<code>pip</code>的情况一样。<br>你可以在root环境下这样使用<code>conda</code>命令，但是conda的python包（与conda可执行文件相对）目前不能在root环境以外的地方安装：</p><pre><code>(myenv) jakevdp$ conda install condaFetching package metadata ...........InstallError: Error: &#39;conda&#39; can only be installed into the root environment</code></pre><p>我猜测在所有conda环境下允许<code>python -m conda install</code>会需要对conda的安装模型做很大的重构工作，因此也许仅仅为了与pip的API对应而做的改动并不划算。<br>即便如此，这样的对应对用户而言也会是很大的帮助。</p><h4 id="A-pip-channel-for-conda"><a href="#A-pip-channel-for-conda" class="headerlink" title="A pip channel for conda?"></a>A pip channel for conda?</h4><p>另一个conda可以做的改动可能是增加一个<a href="https://pypi.python.org/pypi" target="_blank" rel="noopener">Python包目录</a>的镜像通道，这样一来，当你使用<code>conda isntall some-package</code>的时候，计算机会自动像<code>pip</code>一样提取软件包。</p><p>我对conda架构的了解还不够深入，因此我不知道增加这样的特性的难度会有多大。但我<em>毫无疑问</em>具有帮助新手接触Python和/或conda的丰富经验：我有把握说增加这样的特性能够使他们的学习曲线更加平稳。</p><h3 id="New-Jupyter-Magic-Functions"><a href="#New-Jupyter-Magic-Functions" class="headerlink" title="New Jupyter Magic Functions"></a>New Jupyter Magic Functions</h3><p>即使上述这些改动都不可能实现的，我们还是可以通过在Jupyter notebook中引入<code>%pip</code>和<code>%conda</code>魔法命令来简化用户体验。这样的魔法命令会检测当前的kernel并使得软件包能被安装在正确的位置。</p><h4 id="pip-magic"><a href="#pip-magic" class="headerlink" title="pip magic"></a>pip magic</h4><p>举个例子，你可以像下面这样定义一个<code>%pip</code>魔法命令，它将能够在当前kernel中使用：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">from</span> IPython<span class="token punctuation">.</span>core<span class="token punctuation">.</span>magic <span class="token keyword">import</span> register_line_magic@register_line_magic<span class="token keyword">def</span> <span class="token function">pip</span><span class="token punctuation">(</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""Use pip from the current kernel"""</span>    <span class="token keyword">from</span> pip <span class="token keyword">import</span> main    main<span class="token punctuation">(</span>args<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><p>按如下方式运行会在预期的位置安装软件包：</p><pre class=" language-python"><code class="language-python"><span class="token operator">%</span>pip install numpy</code></pre><pre><code>Requirement already satisfied: numpy in /Users/jakevdp/anaconda/lib/python3.6/site-packages</code></pre><p>​    </p><p>值得一提的是，Jupyter开发者Matthias Bussonnier已经在他的<a href="https://github.com/Carreau/pip_magic" target="_blank" rel="noopener">pip_magic</a>代码仓库中发表了这样的内容，因此你可以通过</p><pre><code>$ python -m pip install pip_magic</code></pre><p>安装，并且现在就使用它（当然，前提是你能够把<code>pip_magic</code>安装在正确的位置！）</p><h4 id="conda-magic"><a href="#conda-magic" class="headerlink" title="conda magic"></a>conda magic</h4><p>类似地，我们可以定义一个这样一个conda魔法命令：如果你输入<code>%conda install XXX</code>，它也能帮你在正确的位置完成安装。<br>这会比<code>pip</code>魔法命令更复杂一点，因为它必须保证当前的环境是兼容conda的，并且（与之前所说的<code>python -m conda install</code>无法使用的问题相关）必须唤起一个子进程来执行正确的shell命令：</p><pre class=" language-python"><code class="language-python"><span class="token keyword">from</span> IPython<span class="token punctuation">.</span>core<span class="token punctuation">.</span>magic <span class="token keyword">import</span> register_line_magic<span class="token keyword">import</span> sys<span class="token keyword">import</span> os<span class="token keyword">from</span> subprocess <span class="token keyword">import</span> Popen<span class="token punctuation">,</span> PIPE<span class="token keyword">def</span> <span class="token function">is_conda_environment</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""Return True if the current Python executable is in a conda env"""</span>    <span class="token comment" spellcheck="true"># TODO: make this work with Conda.exe in Windows</span>    conda_exec <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>dirname<span class="token punctuation">(</span>sys<span class="token punctuation">.</span>executable<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'conda'</span><span class="token punctuation">)</span>    conda_history <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>sys<span class="token punctuation">.</span>prefix<span class="token punctuation">,</span> <span class="token string">'conda-meta'</span><span class="token punctuation">,</span> <span class="token string">'history'</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span>conda_exec<span class="token punctuation">)</span> <span class="token operator">and</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span>conda_history<span class="token punctuation">)</span>@register_line_magic<span class="token keyword">def</span> <span class="token function">conda</span><span class="token punctuation">(</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""Use conda from the current kernel"""</span>    <span class="token comment" spellcheck="true"># TODO: make this work with Conda.exe in Windows</span>    <span class="token comment" spellcheck="true"># TODO: fix string encoding to work with Python 2</span>    <span class="token keyword">if</span> <span class="token operator">not</span> is_conda_environment<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"The python kernel does not appear to be a conda environment.  "</span>                         <span class="token string">"Please use ``%pip install`` instead."</span><span class="token punctuation">)</span>    conda_executable <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>dirname<span class="token punctuation">(</span>sys<span class="token punctuation">.</span>executable<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'conda'</span><span class="token punctuation">)</span>    args <span class="token operator">=</span> <span class="token punctuation">[</span>conda_executable<span class="token punctuation">]</span> <span class="token operator">+</span> args<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># Add --prefix to point conda installation to the current environment</span>    <span class="token keyword">if</span> args<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">'install'</span><span class="token punctuation">,</span> <span class="token string">'update'</span><span class="token punctuation">,</span> <span class="token string">'upgrade'</span><span class="token punctuation">,</span> <span class="token string">'remove'</span><span class="token punctuation">,</span> <span class="token string">'uninstall'</span><span class="token punctuation">,</span> <span class="token string">'list'</span><span class="token punctuation">]</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> <span class="token string">'-p'</span> <span class="token operator">not</span> <span class="token keyword">in</span> args <span class="token operator">and</span> <span class="token string">'--prefix'</span> <span class="token operator">not</span> <span class="token keyword">in</span> args<span class="token punctuation">:</span>            args<span class="token punctuation">.</span>insert<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token string">'--prefix'</span><span class="token punctuation">)</span>            args<span class="token punctuation">.</span>insert<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> sys<span class="token punctuation">.</span>prefix<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># Because the notebook does not allow us to respond "yes" during the</span>    <span class="token comment" spellcheck="true"># installation, we need to insert --yes in the argument list for some commands</span>    <span class="token keyword">if</span> args<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">'install'</span><span class="token punctuation">,</span> <span class="token string">'update'</span><span class="token punctuation">,</span> <span class="token string">'upgrade'</span><span class="token punctuation">,</span> <span class="token string">'remove'</span><span class="token punctuation">,</span> <span class="token string">'uninstall'</span><span class="token punctuation">,</span> <span class="token string">'create'</span><span class="token punctuation">]</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> <span class="token string">'-y'</span> <span class="token operator">not</span> <span class="token keyword">in</span> args <span class="token operator">and</span> <span class="token string">'--yes'</span> <span class="token operator">not</span> <span class="token keyword">in</span> args<span class="token punctuation">:</span>            args<span class="token punctuation">.</span>insert<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token string">'--yes'</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># Call conda from command line with subprocess &amp; send results to stdout &amp; stderr</span>    <span class="token keyword">with</span> Popen<span class="token punctuation">(</span>args<span class="token punctuation">,</span> stdout<span class="token operator">=</span>PIPE<span class="token punctuation">,</span> stderr<span class="token operator">=</span>PIPE<span class="token punctuation">)</span> <span class="token keyword">as</span> process<span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># Read stdout character by character, as it includes real-time progress updates</span>        <span class="token keyword">for</span> c <span class="token keyword">in</span> iter<span class="token punctuation">(</span><span class="token keyword">lambda</span><span class="token punctuation">:</span> process<span class="token punctuation">.</span>stdout<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> b<span class="token string">''</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            sys<span class="token punctuation">.</span>stdout<span class="token punctuation">.</span>write<span class="token punctuation">(</span>c<span class="token punctuation">.</span>decode<span class="token punctuation">(</span>sys<span class="token punctuation">.</span>stdout<span class="token punctuation">.</span>encoding<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># Read stderr line by line, because real-time does not matter</span>        <span class="token keyword">for</span> line <span class="token keyword">in</span> iter<span class="token punctuation">(</span>process<span class="token punctuation">.</span>stderr<span class="token punctuation">.</span>readline<span class="token punctuation">,</span> b<span class="token string">''</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            sys<span class="token punctuation">.</span>stderr<span class="token punctuation">.</span>write<span class="token punctuation">(</span>line<span class="token punctuation">.</span>decode<span class="token punctuation">(</span>sys<span class="token punctuation">.</span>stderr<span class="token punctuation">.</span>encoding<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><p>You can now use <code>%conda install</code> and it will install packages to the correct environment:</p><p>现在你就可以使用<code>%conda install</code>在正确环境下安装软件包了：</p><pre class=" language-python"><code class="language-python"><span class="token operator">%</span>conda install numpy</code></pre><pre><code>Fetching package metadata ...........Solving package specifications: .# All requested packages already installed.# packages in environment at /Users/jakevdp/anaconda:#numpy                     1.13.3           py36h2cdce51_0  </code></pre><p>这个conda魔法命令还需要一些工作才能用作一个通用的解决办法（参考代码中的TODO注释），但我认为这是一个很有意义的开始。</p><p>如果这样的pip和conda魔法命令能够加入Jupyter默认的魔法命令集，我认为它将在解决在Jupyter notebook中安装Python包的道路上迈出一大步。<br>然而这样的方法也不是完全安全的：这几个魔法命令只不过是另一层抽象而已，像一切抽象一样，它们也不可避免地存在泄漏的问题。<br>但如果能很谨慎地实现它们，我认为将大大改善用户体验。</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>在这篇博文中，我试图一次性解答这个多年来盛久不衰的问题，<em>我要怎么在Jupyter notebook里安装Python包？</em></p><p>在提出了一些可以实现的简单解决办法后，我深入探究了<em>为什么</em>我们需要这么做：这归根结底来源于Jupyter的kernel是与shell不相连的。</p><p>kernel环境在执行期间可能会改变，但shell环境在Jupyter开始运行时就已经被确定了。<br>一个完整的解释需要花费如此巨大的篇幅，并且需要引入这么多概念，在我看来已经揭示了Jupyter生态的一个真实存在的缺陷，因此我为社区开发者们提出了一些可行的办法，来优化用户体验。</p><p>最后必须补充一点：我对Jupyter，conda，pip和其他一些构成了Python数据科学生态的工具的开发者们具有崇高的尊重和敬意。<br>我非常确定，这些开发者们此前就考虑过这些问题，并且权衡过这些可能的修复办法——如果你读到了这篇文章，请不要有所顾忌，我非常欢迎来自您的评论，也许您能指出一些我忽略了的东西。<br>最后，感谢你们为开源社区所做的贡献。</p><p><em>Thanks to Andy Mueller, Craig Citro, and Matthias Bussonnier for helpful comments on an early draft of this post</em>.</p><p><em>This post was written within a Jupyter notebook; you can view a static version <a href="http://nbviewer.jupyter.org/url/jakevdp.github.com/downloads/notebooks/JupyterInstallation.ipynb" target="_blank" rel="noopener">here</a> or download the full notebook <a href="http://jakevdp.github.com/downloads/notebooks/JupyterInstallation.ipynb" target="_blank" rel="noopener">here</a>.</em></p><p>感谢原作者提供翻译许可。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Installing-Python-Packages-from-a-Jupyter-Notebook&quot;&gt;&lt;a href=&quot;#Installing-Python-Packages-from-a-Jupyter-Notebook&quot; class=&quot;headerlink&quot;</summary>
      
    
    
    
    <category term="tech" scheme="https://huangweiran.club/categories/tech/"/>
    
    
    <category term="tutorial" scheme="https://huangweiran.club/tags/tutorial/"/>
    
    <category term="translation" scheme="https://huangweiran.club/tags/translation/"/>
    
  </entry>
  
</feed>
